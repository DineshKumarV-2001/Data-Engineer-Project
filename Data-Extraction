# **Setting UP Spark Environment**

  

**1\. Deploy Spark Cluster** 

The Apache Spark cluster was deployed on Dataproc - Google Cloud Platform (GCP) using the GCP SDK with automated scripts based on the code provided. This setup enabled a scalable and managed Spark environment suitable for processing large datasets.

gcloud dataproc clusters create my-cluster --region=us-central1 --zone=us-central1-a --master-machine-type=e2-standard-4 --master-boot-disk-size=50GB --num-workers=2 --worker-machine-type=e2-standard-4 --worker-boot-disk-size=50GB --image-version=2.1-debian11 --enable-component-gateway --optional-components=JUPYTER,ZEPPELIN --properties="spark:spark.ui.port=0" --metadata="PIP_PACKAGES=pandas numpy matplotlib seaborn scikit-learn" --project=keen-truth-461516-a0


**2\. Data Extraction and Local Directory Setup**  

A new directory named `olist` was created to organize the project files for the data engineering workflow. The [**Olist Brazilian E-commerce dataset**](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce) was downloaded directly from **Kaggle** using a `curl` command within a Bash script, executed through a remote SSH terminal.

  

#!/bin/bash curl -L -o ~/olist/brazilian-ecommerce.zip https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce

  

After downloading the dataset:

*   The ZIP file was extracted using standard unzip cmd (**unzip brazilian-ecommerce.zip -d ~/olist/dataset/**).

*   A new subdirectory named `dataset` was created inside the main `olist` directory.
    
*   All the extracted `.csv` files were moved into the `dataset` folder for structured data storage and easier access during the ETL process.

**3. Data Injection to Hadoop Cluster**
The olist datasets was injected into the Hadoop Distributed File System (HDFS) to enable distributed storage and parallel processing using the Hadoop ecosystem.

*  üìÅ HDFS Directory Creation:
A new directory was created in HDFS to store the CSV files related to the Olist project: **hadoop fs -mkdir -p /data/olist/**

* üì§Uploading CSV Files to HDFS:
All CSV files from the local project directory (~/olist/dataset) were copied to the HDFS directory /data/olist using the hadoop fs -put command: **hadoop fs -put ~/olist/dataset/*.csv /data/olist/**
