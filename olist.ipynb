{"cells": [{"cell_type": "markdown", "id": "da160ca9-c756-44b6-b05f-1cb2eda7136d", "metadata": {}, "source": "# **Setting UP Spark Environment**\n\n  \n\n**1\\. Deploy Spark Cluster** \n\nThe Apache Spark cluster was deployed on Dataproc - Google Cloud Platform (GCP) using the GCP SDK with automated scripts based on the code provided. This setup enabled a scalable and managed Spark environment suitable for processing large datasets.\n\ngcloud dataproc clusters create my-cluster --region=us-central1 --zone=us-central1-a --master-machine-type=e2-standard-4 --master-boot-disk-size=50GB --num-workers=2 --worker-machine-type=e2-standard-4 --worker-boot-disk-size=50GB --image-version=2.1-debian11 --enable-component-gateway --optional-components=JUPYTER,ZEPPELIN --properties=\"spark:spark.ui.port=0\" --metadata=\"PIP_PACKAGES=pandas numpy matplotlib seaborn scikit-learn\" --project=keen-truth-461516-a0\n\n\n**2\\. Data Extraction and Local Directory Setup**  \n\nA new directory named `olist` was created to organize the project files for the data engineering workflow. The [**Olist Brazilian E-commerce dataset**](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce) was downloaded directly from **Kaggle** using a `curl` command within a Bash script, executed through a remote SSH terminal.\n\n  \n\n#!/bin/bash curl -L -o ~/olist/brazilian-ecommerce.zip https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce\n\n  \n\nAfter downloading the dataset:\n\n*   The ZIP file was extracted using standard unzip cmd (**unzip brazilian-ecommerce.zip -d ~/olist/dataset/**).\n\n*   A new subdirectory named `dataset` was created inside the main `olist` directory.\n    \n*   All the extracted `.csv` files were moved into the `dataset` folder for structured data storage and easier access during the ETL process.\n\n**3. Data Injection to Hadoop Cluster**\nThe olist datasets was injected into the Hadoop Distributed File System (HDFS) to enable distributed storage and parallel processing using the Hadoop ecosystem.\n\n*  \ud83d\udcc1 HDFS Directory Creation:\nA new directory was created in HDFS to store the CSV files related to the Olist project: **hadoop fs -mkdir -p /data/olist/**\n\n* \ud83d\udce4Uploading CSV Files to HDFS:\nAll CSV files from the local project directory (~/olist/dataset) were copied to the HDFS directory /data/olist using the hadoop fs -put command: **hadoop fs -put ~/olist/dataset/*.csv /data/olist/**"}, {"cell_type": "markdown", "id": "74ac85ab-05b9-4e10-8334-a79e157b5e6c", "metadata": {}, "source": "# Data Exploration"}, {"cell_type": "code", "execution_count": 1, "id": "0e68a80c-5c55-445f-909a-e13ff22c3fc3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/08 05:58:09 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-cluster-m.us-central1-a.c.keen-truth-461516-a0.internal:38697\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fc5904a1870>"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n.appName('OlistDataset') \\\n.getOrCreate()\n\nspark"}, {"cell_type": "code", "execution_count": 4, "id": "bf436576-160c-4db2-80b2-46dbbad29854", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 9 items\n-rw-r--r--   2 dgclub21 hadoop    9033957 2025-06-07 12:21 /data/olist/olist_customers_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop   61273883 2025-06-07 12:21 /data/olist/olist_geolocation_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop   15438671 2025-06-07 12:21 /data/olist/olist_order_items_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop    5777138 2025-06-07 12:21 /data/olist/olist_order_payments_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop   14451670 2025-06-07 12:21 /data/olist/olist_order_reviews_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop   17654914 2025-06-07 12:21 /data/olist/olist_orders_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop    2379446 2025-06-07 12:21 /data/olist/olist_products_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop     174703 2025-06-07 12:21 /data/olist/olist_sellers_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop       2613 2025-06-07 12:21 /data/olist/product_category_name_translation.csv\n"}], "source": "!hadoop fs -ls /data/olist/"}, {"cell_type": "code", "execution_count": 3, "id": "cc3ce3f6-dbf8-428f-b697-8e2dc927810c", "metadata": {}, "outputs": [], "source": "hdfs_path = '/data/olist/'      # This approach allows easy access to all dataset files by simply updating the filename in the path."}, {"cell_type": "code", "execution_count": 4, "id": "bf3135c2-c072-4ca5-b537-19ea8b772ebd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+------------------------+--------------------+--------------+\n|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|\n+--------------------+--------------------+------------------------+--------------------+--------------+\n|06b8999e2fba1a1fb...|861eff4711a542e4b...|                   14409|              franca|            SP|\n|18955e83d337fd6b2...|290c77bc529b7ac93...|                    9790|sao bernardo do c...|            SP|\n|4e7b3e00288586ebd...|060e732b5b29e8181...|                    1151|           sao paulo|            SP|\n|b2b6027bc5c5109e5...|259dac757896d24d7...|                    8775|     mogi das cruzes|            SP|\n|4f2d8ab171c80ec83...|345ecd01c38d18a90...|                   13056|            campinas|            SP|\n+--------------------+--------------------+------------------------+--------------------+--------------+\nonly showing top 5 rows\n\n"}], "source": "customer_df = spark.read.csv(hdfs_path + 'olist_customers_dataset.csv',header=True,inferSchema=True)\ncustomer_df.show(5)"}, {"cell_type": "code", "execution_count": 64, "id": "9301603d-c591-44f1-a78e-5252568bf87f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "location_df = spark.read.csv(hdfs_path + 'olist_geolocation_dataset.csv',header=True,inferSchema=True)\nitem_df = spark.read.csv(hdfs_path + 'olist_order_items_dataset.csv',header=True,inferSchema=True)\npayment_df = spark.read.csv(hdfs_path + 'olist_order_payments_dataset.csv',header=True,inferSchema=True)\nreview_df = spark.read.csv(hdfs_path + 'olist_order_reviews_dataset.csv',header=True,inferSchema=True)\norder_df = spark.read.csv(hdfs_path + 'olist_orders_dataset.csv',header=True,inferSchema=True)\nproduct_df = spark.read.csv(hdfs_path + 'olist_products_dataset.csv',header=True,inferSchema=True)\nseller_df = spark.read.csv(hdfs_path + 'olist_sellers_dataset.csv',header=True,inferSchema=True)\ntranslation_df = spark.read.csv(hdfs_path + 'product_category_name_translation.csv',header=True,inferSchema=True)"}, {"cell_type": "code", "execution_count": 15, "id": "573e0e9f-57bc-4a54-b6f1-dd308f7ef760", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: string (nullable = true)\n |-- customer_unique_id: string (nullable = true)\n |-- customer_zip_code_prefix: integer (nullable = true)\n |-- customer_city: string (nullable = true)\n |-- customer_state: string (nullable = true)\n\n"}], "source": "customer_df.printSchema()"}, {"cell_type": "code", "execution_count": 16, "id": "c4f8198d-1b14-49cd-9230-8b52a1519ddc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- geolocation_zip_code_prefix: integer (nullable = true)\n |-- geolocation_lat: double (nullable = true)\n |-- geolocation_lng: double (nullable = true)\n |-- geolocation_city: string (nullable = true)\n |-- geolocation_state: string (nullable = true)\n\n"}], "source": "location_df.printSchema()"}, {"cell_type": "code", "execution_count": 15, "id": "af06c322-f70d-4167-be44-4a8abe05bcb2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- payment_sequential: integer (nullable = true)\n |-- payment_type: string (nullable = true)\n |-- payment_installments: integer (nullable = true)\n |-- payment_value: double (nullable = true)\n\n"}], "source": "payment_df.printSchema()"}, {"cell_type": "markdown", "id": "6e38bf37-0e31-4594-adf9-3aa782dd4c03", "metadata": {}, "source": "## Data Validation \u2013 Row Count Check"}, {"cell_type": "markdown", "id": "dbe30ad2-e346-4826-a22d-11b7b9031b56", "metadata": {}, "source": "checking for Data Leakage (any data is missed during Extraction or in Injection Process)."}, {"cell_type": "code", "execution_count": 22, "id": "c8ae2eea-e663-4438-8b68-99a08b8e3406", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Customer_df : 99441 rows\nlocation_df : 1000163 rows\nitem_df     : 112650 rows\npayment_df  : 103886 rows\nreview_df   : 104162 rows\norder_df    : 99441 rows\nproduct_df  : 32951 rows\nseller_df   : 3095 rows\ntranslation_df : 71 rows\n"}], "source": "print(f'Customer_df : {customer_df.count()} rows')\nprint(f'location_df : {location_df.count()} rows')\nprint(f'item_df     : {item_df.count()} rows')\nprint(f'payment_df  : {payment_df.count()} rows')\nprint(f'review_df   : {review_df.count()} rows')\nprint(f'order_df    : {order_df.count()} rows')\nprint(f'product_df  : {product_df.count()} rows')\nprint(f'seller_df   : {seller_df.count()} rows')\nprint(f'translation_df : {translation_df.count()} rows')"}, {"cell_type": "code", "execution_count": 24, "id": "eb1fa3c9-91b3-4fb2-9dd6-adf3f781187b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "103886"}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": "payment_df.distinct().count()"}, {"cell_type": "markdown", "id": "9d6a4779-e96b-41f3-9043-6a4973848471", "metadata": {}, "source": "## Null Validation"}, {"cell_type": "code", "execution_count": 30, "id": "1caba614-5391-48d3-a6a4-ae8423f71c3b", "metadata": {}, "outputs": [{"data": {"text/plain": "['customer_id',\n 'customer_unique_id',\n 'customer_zip_code_prefix',\n 'customer_city',\n 'customer_state']"}, "execution_count": 30, "metadata": {}, "output_type": "execute_result"}], "source": "customer_df.columns"}, {"cell_type": "code", "execution_count": 43, "id": "a0045b1c-72b0-41b6-806a-c7e016e663f0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------------+----------------------------+----------------------------------+-----------------------+------------------------+\n|(customer_id IS NULL)|(customer_unique_id IS NULL)|(customer_zip_code_prefix IS NULL)|(customer_city IS NULL)|(customer_state IS NULL)|\n+---------------------+----------------------------+----------------------------------+-----------------------+------------------------+\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n|                false|                       false|                             false|                  false|                   false|\n+---------------------+----------------------------+----------------------------------+-----------------------+------------------------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql.functions import col\n\ncustomer_df.select([col(i).isNull() for i in customer_df.columns]).show()"}, {"cell_type": "code", "execution_count": 14, "id": "7bab5d9e-da34-4763-86ef-ed40539ea5d8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+------------------+------------------------+-------------+--------------+\n|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|\n+-----------+------------------+------------------------+-------------+--------------+\n|          0|                 0|                       0|            0|             0|\n+-----------+------------------+------------------------+-------------+--------------+\n\n"}], "source": "from pyspark.sql.functions import col,when,count,lit\n\ncustomer_df.select([count(when(col(c).isNull(),1)).alias(c) for c in customer_df.columns]).show()"}, {"cell_type": "code", "execution_count": 38, "id": "d2bd0794-e29d-4139-8458-100ff379a615", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "-RECORD 0-----------------------------\n order_id                      | 0    \n customer_id                   | 0    \n order_status                  | 0    \n order_purchase_timestamp      | 0    \n order_approved_at             | 160  \n order_delivered_carrier_date  | 1783 \n order_delivered_customer_date | 2965 \n order_estimated_delivery_date | 0    \n\n"}], "source": "from pyspark.sql.functions import col,when,count\n\norder_df.select([count(when(col(c).isNull(),1)).alias(c) for c in order_df.columns]).show(vertical=True)"}, {"cell_type": "markdown", "id": "0ef49480-1faf-4f36-801e-90ff1edcf5a9", "metadata": {}, "source": "## Duplicate Validation"}, {"cell_type": "code", "execution_count": 46, "id": "b3fc997d-0bfd-4f44-a56d-8a65e715b6ff", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 60:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----+\n|  customer_unique_id|count|\n+--------------------+-----+\n|8d50f5eadf50201cc...|   17|\n|3e43e6105506432c9...|    9|\n|ca77025e7201e3b30...|    7|\n|1b6c7548a2a1f9037...|    7|\n|6469f99c1f9dfae77...|    7|\n|f0e310a6839dce9de...|    6|\n|12f5d6e1cbf93dafd...|    6|\n|dc813062e0fc23409...|    6|\n|47c1a3033b8b77b3a...|    6|\n|de34b16117594161a...|    6|\n|63cfc61cee11cbe30...|    6|\n|56c8638e7c058b98a...|    5|\n|394ac4de8f3acb142...|    5|\n|5e8f38a9a1c023f3d...|    5|\n|74cb1ad7e6d567432...|    5|\n|b4e4f24de1e8725b7...|    5|\n|35ecdf6858edc6427...|    5|\n|fe81bb32c243a86b2...|    5|\n|4e65032f1f574189f...|    5|\n|9cc5a07f169a1606f...|    4|\n+--------------------+-----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customer_df.groupBy('customer_unique_id').count().filter('count>1').orderBy('count',ascending=False).show()"}, {"cell_type": "code", "execution_count": 6, "id": "8f819cf0-3366-43fd-b5b8-de0b7c17c5ba", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 19:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------+-----+\n|customer_state|count|\n+--------------+-----+\n|            SP|41746|\n|            RJ|12852|\n|            MG|11635|\n|            RS| 5466|\n|            PR| 5045|\n|            SC| 3637|\n|            BA| 3380|\n|            DF| 2140|\n|            ES| 2033|\n|            GO| 2020|\n|            PE| 1652|\n|            CE| 1336|\n|            PA|  975|\n|            MT|  907|\n|            MA|  747|\n|            MS|  715|\n|            PB|  536|\n|            PI|  495|\n|            RN|  485|\n|            AL|  413|\n+--------------+-----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Customers Distribution by State\n\nfrom pyspark.sql.functions import desc\n\ncustomer_df.groupBy('customer_state').count().orderBy(desc('count')).show()"}, {"cell_type": "code", "execution_count": 7, "id": "52adbb4e-0406-40c0-9ce5-fea3f7872294", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 22:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------+-----+\n|order_status|count|\n+------------+-----+\n|   delivered|96478|\n|     shipped| 1107|\n|    canceled|  625|\n| unavailable|  609|\n|    invoiced|  314|\n|  processing|  301|\n|     created|    5|\n|    approved|    2|\n+------------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Order : Order Status Distribution\n\norder_df.groupBy('order_status').count().orderBy('count',ascending=False).show()"}, {"cell_type": "code", "execution_count": 9, "id": "25aa0330-9d9f-4e67-80fd-d5f5a1b3425a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+-----+\n|payment_type|count|\n+------------+-----+\n| credit_card|76795|\n|      boleto|19784|\n|     voucher| 5775|\n|  debit_card| 1529|\n| not_defined|    3|\n+------------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Payments : Finding Hieghest Payment type\n\npayment_df.groupBy('payment_type').count().orderBy('count',ascending=False).show()"}, {"cell_type": "code", "execution_count": 8, "id": "4bbc83aa-0290-4d1f-b70d-8511b5df7115", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+------------+--------------------+-------------+\n|            order_id|payment_sequential|payment_type|payment_installments|payment_value|\n+--------------------+------------------+------------+--------------------+-------------+\n|b81ef226f3fe1789b...|                 1| credit_card|                   8|        99.33|\n|a9810da82917af2d9...|                 1| credit_card|                   1|        24.39|\n|25e8ea4e93396b6fa...|                 1| credit_card|                   1|        65.71|\n+--------------------+------------------+------------+--------------------+-------------+\nonly showing top 3 rows\n\n"}], "source": "payment_df.show(3)"}, {"cell_type": "code", "execution_count": 27, "id": "8451ac31-7143-47e9-9dfa-52b293073ac4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 38:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----------+\n|          product_id|total_sales|\n+--------------------+-----------+\n|bb50f2e236e5eea01...|    63885.0|\n|6cdd53843498f9289...|    54730.0|\n|d6160fb7873f18409...|    48899.0|\n|d1c427060a0f73f6b...|    47215.0|\n|99a4788cb24856965...|    43026.0|\n+--------------------+-----------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Order Items : Finding Top Selling Products\n\nfrom pyspark.sql.functions import sum,round\n\ntop_products = item_df.groupBy('product_id').agg(round(sum('price')).alias('total_sales'))\ntop_products.orderBy('total_sales',ascending=False).show(5)"}, {"cell_type": "code", "execution_count": 39, "id": "589e8357-a638-44ee-ac9b-5f70a60bd0dc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------------+-----------------------------+\n|            order_id|order_purchase_timestamp|order_delivered_customer_date|\n+--------------------+------------------------+-----------------------------+\n|e481f51cbdc54678b...|     2017-10-02 10:56:33|          2017-10-10 21:25:13|\n|53cdb2fc8bc7dce0b...|     2018-07-24 20:41:37|          2018-08-07 15:27:45|\n|47770eb9100c2d0c4...|     2018-08-08 08:38:49|          2018-08-17 18:06:29|\n|949d5b44dbf5de918...|     2017-11-18 19:28:06|          2017-12-02 00:28:42|\n|ad21c59c0840e6cb8...|     2018-02-13 21:18:39|          2018-02-16 18:17:02|\n+--------------------+------------------------+-----------------------------+\nonly showing top 5 rows\n\n"}], "source": "# Orders : Average Delivery Time Analysis\n\ndelivery_df = order_df.select('order_id','order_purchase_timestamp','order_delivered_customer_date')\ndelivery_df.show(5)"}, {"cell_type": "code", "execution_count": 41, "id": "42d63c15-e6c8-40b2-aaac-6475b3f26244", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 51:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------------+-----------------------------+-------------+\n|            order_id|order_purchase_timestamp|order_delivered_customer_date|delivery_time|\n+--------------------+------------------------+-----------------------------+-------------+\n|ca07593549f1816d2...|     2017-02-21 23:31:27|          2017-09-19 14:36:39|          210|\n|1b3190b2dfa9d789e...|     2018-02-23 14:57:35|          2018-09-19 23:24:07|          208|\n|440d0d17af552815d...|     2017-03-07 23:59:51|          2017-09-19 15:12:50|          196|\n|2fb597c2f772eca01...|     2017-03-08 18:09:02|          2017-09-19 14:33:17|          195|\n|285ab9426d6982034...|     2017-03-08 22:47:40|          2017-09-19 14:00:04|          195|\n|0f4519c5f1c541dde...|     2017-03-09 13:26:57|          2017-09-19 14:38:21|          194|\n|47b40429ed8cce3ae...|     2018-01-03 09:44:01|          2018-07-13 20:51:31|          191|\n|2fe324febf907e3ea...|     2017-03-13 20:17:10|          2017-09-19 17:00:07|          190|\n|c27815f7e3dd0b926...|     2017-03-15 23:23:17|          2017-09-19 17:14:25|          188|\n|2d7561026d542c8db...|     2017-03-15 11:24:27|          2017-09-19 14:38:18|          188|\n|437222e3fd1b07396...|     2017-03-16 11:36:00|          2017-09-19 16:28:58|          187|\n|dfe5f68118c257614...|     2017-03-17 12:32:22|          2017-09-19 18:13:19|          186|\n|6e82dcfb5eada6283...|     2017-05-17 19:09:02|          2017-11-16 10:56:45|          183|\n|2ba1366baecad3c35...|     2017-02-28 14:56:37|          2017-08-28 16:23:46|          181|\n|d24e8541128cea179...|     2017-06-12 13:14:11|          2017-12-04 18:36:29|          175|\n|3566eabb132f8d647...|     2017-03-29 13:57:55|          2017-09-19 15:07:09|          174|\n|ed8e9faf1b75f43ee...|     2017-11-29 15:10:14|          2018-05-21 18:22:18|          173|\n|2fa29503f2ebd9f53...|     2017-03-31 15:03:51|          2017-09-19 18:24:46|          172|\n|4fbc8d6f2f4db3e78...|     2018-02-02 21:38:36|          2018-07-20 23:37:50|          168|\n|a452fba32eab28a4a...|     2017-04-04 23:21:02|          2017-09-19 13:47:09|          168|\n+--------------------+------------------------+-----------------------------+-------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import datediff,to_date,col\n\ndelivery_time = delivery_df.withColumn('delivery_time',datediff(col('order_delivered_customer_date'),col('order_purchase_timestamp')))\ndelivery_time.orderBy('delivery_time',ascending=False).show()"}, {"cell_type": "markdown", "id": "ed434878-fcdc-4084-9d72-0f5df009010e", "metadata": {}, "source": "# Data Cleaning & Transformation"}, {"cell_type": "code", "execution_count": 3, "id": "7ec38676-03e4-4dcb-a8b0-8e7841616fa1", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/15 10:56:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-cluster-m.us-central1-a.c.keen-truth-461516-a0.internal:39921\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f9f7c64d900>"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName('Data Cleaning & Transformation') \\\n    .getOrCreate()\n\nspark"}, {"cell_type": "code", "execution_count": 7, "id": "5397a1a0-3e2a-49ef-903e-bbd5e90f6dc5", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 9 items\n-rw-r--r--   2 dgclub21 hadoop    9033957 2025-06-07 12:21 /data/olist/olist_customers_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop   61273883 2025-06-07 12:21 /data/olist/olist_geolocation_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop   15438671 2025-06-07 12:21 /data/olist/olist_order_items_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop    5777138 2025-06-07 12:21 /data/olist/olist_order_payments_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop   14451670 2025-06-07 12:21 /data/olist/olist_order_reviews_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop   17654914 2025-06-07 12:21 /data/olist/olist_orders_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop    2379446 2025-06-07 12:21 /data/olist/olist_products_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop     174703 2025-06-07 12:21 /data/olist/olist_sellers_dataset.csv\n-rw-r--r--   2 dgclub21 hadoop       2613 2025-06-07 12:21 /data/olist/product_category_name_translation.csv\n"}], "source": "!hadoop fs -ls /data/olist/"}, {"cell_type": "code", "execution_count": 1, "id": "0d6a77b7-90df-4917-9dcb-86da806cf276", "metadata": {}, "outputs": [], "source": "hdfs_path = '/data/olist/'"}, {"cell_type": "code", "execution_count": 2, "id": "2aaaf739-3c23-47b1-bac5-8d4bf9b41087", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customer_df = spark.read.csv(hdfs_path + 'olist_customers_dataset.csv',header=True,inferSchema=True)\nlocation_df = spark.read.csv(hdfs_path + 'olist_geolocation_dataset.csv',header=True,inferSchema=True)\nitem_df = spark.read.csv(hdfs_path + 'olist_order_items_dataset.csv',header=True,inferSchema=True)\npayment_df = spark.read.csv(hdfs_path + 'olist_order_payments_dataset.csv',header=True,inferSchema=True)\nreview_df = spark.read.csv(hdfs_path + 'olist_order_reviews_dataset.csv',header=True,inferSchema=True)\norder_df = spark.read.csv(hdfs_path + 'olist_orders_dataset.csv',header=True,inferSchema=True)\nproduct_df = spark.read.csv(hdfs_path + 'olist_products_dataset.csv',header=True,inferSchema=True)\nseller_df = spark.read.csv(hdfs_path + 'olist_sellers_dataset.csv',header=True,inferSchema=True)\ntranslation_df = spark.read.csv(hdfs_path + 'product_category_name_translation.csv',header=True,inferSchema=True)"}, {"cell_type": "markdown", "id": "62211e67-2a63-4795-9d75-7eb563cba298", "metadata": {}, "source": "## Finding Missing Values"}, {"cell_type": "code", "execution_count": 15, "id": "761b0a74-02d0-46e8-9a7c-1016e61a7b6a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "-RECORD 0-----------------------\n customer_id              | 0   \n customer_unique_id       | 0   \n customer_zip_code_prefix | 0   \n customer_city            | 0   \n customer_state           | 0   \n\n"}], "source": "from pyspark.sql.functions import *\n\ncustomer_df.select([count(when(col(c).isNull(),1)).alias(c) for c in customer_df.columns]).show(vertical=True)"}, {"cell_type": "markdown", "id": "01883a9f-82ef-4226-8abd-6691b156136b", "metadata": {}, "source": "## Function to Find Null's"}, {"cell_type": "code", "execution_count": 26, "id": "464ba285-c110-4373-8727-6da46f76cae9", "metadata": {}, "outputs": [], "source": "def missing(df,table_name):\n    print(f'Missing Fields From : {table_name}')\n    df.select([count(when(col(c).isNull(),1)).alias(c) for c in df.columns]).show(vertical=True)"}, {"cell_type": "code", "execution_count": 29, "id": "339d882f-2925-4fcf-b0ae-a4aecd672946", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Missing Fields From : Sellers Table\n-RECORD 0---------------------\n seller_id              | 0   \n seller_zip_code_prefix | 0   \n seller_city            | 0   \n seller_state           | 0   \n\n"}], "source": "missing(seller_df,'Sellers Table')"}, {"cell_type": "code", "execution_count": 28, "id": "d5fb6058-7e96-42ee-a9dd-943a9aeaf901", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Missing Fields From : Payments Table\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 48:>                                                         (0 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "-RECORD 0-------------------\n order_id             | 0   \n payment_sequential   | 0   \n payment_type         | 0   \n payment_installments | 0   \n payment_value        | 0   \n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "missing(payment_df,'Payments Table')"}, {"cell_type": "code", "execution_count": 27, "id": "67697720-e78d-4e3b-8d8a-49fc3c0d5308", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Missing Fields From : Orders Table\n-RECORD 0-----------------------------\n order_id                      | 0    \n customer_id                   | 0    \n order_status                  | 0    \n order_purchase_timestamp      | 0    \n order_approved_at             | 160  \n order_delivered_carrier_date  | 1783 \n order_delivered_customer_date | 2965 \n order_estimated_delivery_date | 0    \n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "missing(order_df,'Orders Table')"}, {"cell_type": "markdown", "id": "be8c772f-0395-44c2-9e92-4dc192ee627c", "metadata": {}, "source": "## Handling Missing Values\n\n1. Drop Missing Values : for Non-Critical Columns.\n\n2. Fill Missing Values : for Numerical Columns , Referencable from any other Column Values.\n\n3. Impute Missing Values : for Continouse Data."}, {"cell_type": "code", "execution_count": 31, "id": "4f0b0196-1067-4c08-8358-6629fe20d446", "metadata": {}, "outputs": [], "source": "order_df.createOrReplaceTempView('orders')"}, {"cell_type": "code", "execution_count": 36, "id": "7f2c4c05-f0b2-4425-88cd-14164347c2d1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+\n|count(1)|\n+--------+\n|     160|\n+--------+\n\n"}], "source": "spark.sql('select count(*) from orders where order_approved_at is null').show()"}, {"cell_type": "markdown", "id": "7acda188-ccf9-40b9-9cbe-b67537732cc8", "metadata": {}, "source": "## Droping Null Columns"}, {"cell_type": "code", "execution_count": 40, "id": "0e79c1a6-696d-4bc5-a778-bb3574175e00", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Count Before Clean :  99441\nCount After Clean :  99281\nCount Difference :  160\n"}], "source": "cleaned_order = order_df.na.drop(subset = ['order_id','customer_id','order_approved_at'])\nprint('Count Before Clean : ', order_df.count());\nprint('Count After Clean : ', cleaned_order.count());\nprint('Count Difference : ', (order_df.count())-(cleaned_order.count()))"}, {"cell_type": "markdown", "id": "53f434db-72a6-40a1-8f4b-d329643db256", "metadata": {}, "source": "## Filling Null Columns "}, {"cell_type": "code", "execution_count": 48, "id": "3568c3c6-77be-4225-8272-f4dd8af42d1b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n|            order_id|         customer_id|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|\n+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n|e481f51cbdc54678b...|9ef432eb625129730...|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|\n|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|   delivered|     2018-07-24 20:41:37|2018-07-26 03:24:27|         2018-07-26 14:31:00|          2018-08-07 15:27:45|          2018-08-13 00:00:00|\n|47770eb9100c2d0c4...|41ce2a54c0b03bf34...|   delivered|     2018-08-08 08:38:49|2018-08-08 08:55:23|         2018-08-08 13:50:00|          2018-08-17 18:06:29|          2018-09-04 00:00:00|\n|949d5b44dbf5de918...|f88197465ea7920ad...|   delivered|     2017-11-18 19:28:06|2017-11-18 19:45:59|         2017-11-22 13:39:59|          2017-12-02 00:28:42|          2017-12-15 00:00:00|\n|ad21c59c0840e6cb8...|8ab97904e6daea886...|   delivered|     2018-02-13 21:18:39|2018-02-13 22:20:29|         2018-02-14 19:46:34|          2018-02-16 18:17:02|          2018-02-26 00:00:00|\n|a4591c265e18cb1dc...|503740e9ca751ccdd...|   delivered|     2017-07-09 21:57:05|2017-07-09 22:10:13|         2017-07-11 14:58:04|          2017-07-26 10:57:55|          2017-08-01 00:00:00|\n|136cce7faa42fdb2c...|ed0271e0b7da060a3...|    invoiced|     2017-04-11 12:22:08|2017-04-13 13:25:17|                        null|                         null|          2017-05-09 00:00:00|\n|6514b8ad8028c9f2c...|9bdf08b4b3b52b552...|   delivered|     2017-05-16 13:10:30|2017-05-16 13:22:11|         2017-05-22 10:07:46|          2017-05-26 12:55:51|          2017-06-07 00:00:00|\n|76c6e866289321a7c...|f54a9f0e6b351c431...|   delivered|     2017-01-23 18:29:09|2017-01-25 02:50:47|         2017-01-26 14:16:31|          2017-02-02 14:08:10|          2017-03-06 00:00:00|\n|e69bfb5eb88e0ed6a...|31ad1d1b63eb99624...|   delivered|     2017-07-29 11:55:02|2017-07-29 12:05:32|         2017-08-10 19:45:24|          2017-08-16 17:14:30|          2017-08-23 00:00:00|\n+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\nonly showing top 10 rows\n\n"}], "source": "order_df.show(10)"}, {"cell_type": "code", "execution_count": 49, "id": "1e60ae68-2277-42f6-b473-29255bcd2b6c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n|            order_id|         customer_id|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|\n+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n|e481f51cbdc54678b...|9ef432eb625129730...|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|\n|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|   delivered|     2018-07-24 20:41:37|2018-07-26 03:24:27|         2018-07-26 14:31:00|          2018-08-07 15:27:45|          2018-08-13 00:00:00|\n|47770eb9100c2d0c4...|41ce2a54c0b03bf34...|   delivered|     2018-08-08 08:38:49|2018-08-08 08:55:23|         2018-08-08 13:50:00|          2018-08-17 18:06:29|          2018-09-04 00:00:00|\n|949d5b44dbf5de918...|f88197465ea7920ad...|   delivered|     2017-11-18 19:28:06|2017-11-18 19:45:59|         2017-11-22 13:39:59|          2017-12-02 00:28:42|          2017-12-15 00:00:00|\n|ad21c59c0840e6cb8...|8ab97904e6daea886...|   delivered|     2018-02-13 21:18:39|2018-02-13 22:20:29|         2018-02-14 19:46:34|          2018-02-16 18:17:02|          2018-02-26 00:00:00|\n|a4591c265e18cb1dc...|503740e9ca751ccdd...|   delivered|     2017-07-09 21:57:05|2017-07-09 22:10:13|         2017-07-11 14:58:04|          2017-07-26 10:57:55|          2017-08-01 00:00:00|\n|136cce7faa42fdb2c...|ed0271e0b7da060a3...|    invoiced|     2017-04-11 12:22:08|2017-04-13 13:25:17|         2017-01-01 00:00:00|          2017-01-01 00:00:00|          2017-05-09 00:00:00|\n|6514b8ad8028c9f2c...|9bdf08b4b3b52b552...|   delivered|     2017-05-16 13:10:30|2017-05-16 13:22:11|         2017-05-22 10:07:46|          2017-05-26 12:55:51|          2017-06-07 00:00:00|\n|76c6e866289321a7c...|f54a9f0e6b351c431...|   delivered|     2017-01-23 18:29:09|2017-01-25 02:50:47|         2017-01-26 14:16:31|          2017-02-02 14:08:10|          2017-03-06 00:00:00|\n|e69bfb5eb88e0ed6a...|31ad1d1b63eb99624...|   delivered|     2017-07-29 11:55:02|2017-07-29 12:05:32|         2017-08-10 19:45:24|          2017-08-16 17:14:30|          2017-08-23 00:00:00|\n+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\nonly showing top 10 rows\n\n"}], "source": "cleaned_order = order_df.fillna({'order_delivered_carrier_date':'2017-01-01' , 'order_delivered_customer_date':'2017-01-01'})\ncleaned_order.show(10)"}, {"cell_type": "markdown", "id": "862e0369-883e-41bb-ade6-c12620923595", "metadata": {}, "source": "## Impute Missing Values"}, {"cell_type": "code", "execution_count": 50, "id": "5bedb510-1eb9-487d-8038-00b00adc4c27", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+------------+--------------------+-------------+\n|            order_id|payment_sequential|payment_type|payment_installments|payment_value|\n+--------------------+------------------+------------+--------------------+-------------+\n|b81ef226f3fe1789b...|                 1| credit_card|                   8|        99.33|\n|a9810da82917af2d9...|                 1| credit_card|                   1|        24.39|\n|25e8ea4e93396b6fa...|                 1| credit_card|                   1|        65.71|\n|ba78997921bbcdc13...|                 1| credit_card|                   8|       107.78|\n|42fdf880ba16b47b5...|                 1| credit_card|                   2|       128.45|\n+--------------------+------------------+------------+--------------------+-------------+\nonly showing top 5 rows\n\n"}], "source": "payment_df.show(5)"}, {"cell_type": "code", "execution_count": 15, "id": "35db9967-aa6a-4be3-b69b-9d912edfbb20", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+------------+--------------------+-------------+\n|            order_id|payment_sequential|payment_type|payment_installments|payment_value|\n+--------------------+------------------+------------+--------------------+-------------+\n|b81ef226f3fe1789b...|                 1| credit_card|                   8|         null|\n|a9810da82917af2d9...|                 1| credit_card|                   1|         null|\n|25e8ea4e93396b6fa...|                 1| credit_card|                   1|         null|\n|ba78997921bbcdc13...|                 1| credit_card|                   8|       107.78|\n|42fdf880ba16b47b5...|                 1| credit_card|                   2|       128.45|\n+--------------------+------------------+------------+--------------------+-------------+\nonly showing top 5 rows\n\n"}], "source": "# there is no null values in payment : so values below 100 we are going to make it null\n\npayment_null = payment_df.withColumn('payment_value',when(col('payment_value')>100,col('payment_value')).otherwise(lit(None)))\npayment_null.show(5)"}, {"cell_type": "code", "execution_count": 16, "id": "538d8bf0-2a9d-4d49-b915-eaf740853848", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.feature import Imputer\n\nimputer = Imputer(inputCols=['payment_value'],outputCols=['payment_values_imputed']).setStrategy('mean')\n\ncleaned_payment = imputer.fit(payment_null).transform(payment_null)"}, {"cell_type": "code", "execution_count": 17, "id": "45fdc0dc-7997-4353-ba74-5cbfa2f657e2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+------------+--------------------+-------------+----------------------+\n|            order_id|payment_sequential|payment_type|payment_installments|payment_value|payment_values_imputed|\n+--------------------+------------------+------------+--------------------+-------------+----------------------+\n|b81ef226f3fe1789b...|                 1| credit_card|                   8|         null|    251.89388114183782|\n|a9810da82917af2d9...|                 1| credit_card|                   1|         null|    251.89388114183782|\n|25e8ea4e93396b6fa...|                 1| credit_card|                   1|         null|    251.89388114183782|\n|ba78997921bbcdc13...|                 1| credit_card|                   8|       107.78|                107.78|\n|42fdf880ba16b47b5...|                 1| credit_card|                   2|       128.45|                128.45|\n+--------------------+------------------+------------+--------------------+-------------+----------------------+\nonly showing top 5 rows\n\n"}], "source": "cleaned_payment.show(5)"}, {"cell_type": "markdown", "id": "9fbe5304-4828-40bb-a940-57ac9eff3bc3", "metadata": {}, "source": "In Apache Spark (PySpark), the Imputer is used to fill in missing values (null/NaN) in numeric columns. It works similarly to scikit-learn\u2019s imputer, allowing strategies like mean, median, or mode"}, {"cell_type": "code", "execution_count": 21, "id": "673fd1a8-4712-438c-b862-0ae72ce516f1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+------------+--------------------+-------------+----------------------+\n|            order_id|payment_sequential|payment_type|payment_installments|payment_value|payment_values_imputed|\n+--------------------+------------------+------------+--------------------+-------------+----------------------+\n|b81ef226f3fe1789b...|                 1| credit_card|                   8|         null|                172.01|\n|a9810da82917af2d9...|                 1| credit_card|                   1|         null|                172.01|\n|25e8ea4e93396b6fa...|                 1| credit_card|                   1|         null|                172.01|\n|ba78997921bbcdc13...|                 1| credit_card|                   8|       107.78|                107.78|\n|42fdf880ba16b47b5...|                 1| credit_card|                   2|       128.45|                128.45|\n|298fcdf1f73eb413e...|                 1| credit_card|                   2|         null|                172.01|\n|771ee386b001f0620...|                 1| credit_card|                   1|         null|                172.01|\n|3d7239c394a212faa...|                 1| credit_card|                   3|         null|                172.01|\n|1f78449c87a54faf9...|                 1| credit_card|                   6|       341.09|                341.09|\n|0573b5e23cbd79800...|                 1|      boleto|                   1|         null|                172.01|\n|d88e0d5fa41661ce0...|                 1| credit_card|                   8|       188.73|                188.73|\n|2480f727e869fdeb3...|                 1| credit_card|                   1|        141.9|                 141.9|\n|616105c9352a9668c...|                 1| credit_card|                   1|         null|                172.01|\n|cf95215a722f3ebf2...|                 1| credit_card|                   5|       102.66|                102.66|\n|769214176682788a9...|                 1| credit_card|                   4|       105.28|                105.28|\n|12e5cfe0e4716b59a...|                 1| credit_card|                  10|       157.45|                157.45|\n|61059985a6fc0ad64...|                 1| credit_card|                   1|       132.04|                132.04|\n|79da3f5fe31ad1e45...|                 1| credit_card|                   1|         null|                172.01|\n|8ac09207f415d55ac...|                 1| credit_card|                   4|       244.15|                244.15|\n|b2349a3f20dfbeef6...|                 1| credit_card|                   3|       136.71|                136.71|\n+--------------------+------------------+------------+--------------------+-------------+----------------------+\nonly showing top 20 rows\n\n"}], "source": "imputer = Imputer(inputCols=['payment_value'],outputCols=['payment_values_imputed']).setStrategy('median')\n\ncleaned_payment = imputer.fit(payment_null).transform(payment_null)\ncleaned_payment.show()"}, {"cell_type": "code", "execution_count": 7, "id": "84b1c639-e22b-4639-b4ed-a125f118c89e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Schema of : Customers\nroot\n |-- customer_id: string (nullable = true)\n |-- customer_unique_id: string (nullable = true)\n |-- customer_zip_code_prefix: integer (nullable = true)\n |-- customer_city: string (nullable = true)\n |-- customer_state: string (nullable = true)\n\n"}], "source": "print_schema(customer_df,'Customers')"}, {"cell_type": "code", "execution_count": 8, "id": "f23301fb-9814-4f9f-8b20-db6b96003a01", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Schema of : Payments\nroot\n |-- order_id: string (nullable = true)\n |-- payment_sequential: integer (nullable = true)\n |-- payment_type: string (nullable = true)\n |-- payment_installments: integer (nullable = true)\n |-- payment_value: double (nullable = true)\n\n"}], "source": "print_schema(payment_df,'Payments')"}, {"cell_type": "code", "execution_count": 14, "id": "56415462-dc9c-4e44-9d35-8f2395955816", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+------------+--------------------+-------------+\n|            order_id|payment_sequential|payment_type|payment_installments|payment_value|\n+--------------------+------------------+------------+--------------------+-------------+\n|b81ef226f3fe1789b...|                 1| credit_card|                   8|        99.33|\n|a9810da82917af2d9...|                 1| credit_card|                   1|        24.39|\n|25e8ea4e93396b6fa...|                 1| credit_card|                   1|        65.71|\n|ba78997921bbcdc13...|                 1| credit_card|                   8|       107.78|\n|42fdf880ba16b47b5...|                 1| credit_card|                   2|       128.45|\n+--------------------+------------------+------------+--------------------+-------------+\nonly showing top 5 rows\n\n"}], "source": "payment_df.show(5)"}, {"cell_type": "code", "execution_count": 17, "id": "053752ee-4fc0-4f64-916f-27d5f2b4281b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 27:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------+-----+\n|payment_type|count|\n+------------+-----+\n| credit_card|76795|\n|      boleto|19784|\n|     voucher| 5775|\n|  debit_card| 1529|\n| not_defined|    3|\n+------------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "payment_df.groupBy('payment_type').count().orderBy('count',ascending=False).show()"}, {"cell_type": "code", "execution_count": 8, "id": "afaa693b-35c6-41f7-b461-33902ee28eac", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+\n|payment_type|\n+------------+\n|       Other|\n|      Boleto|\n|  Debit Card|\n| Credit Card|\n+------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import when,length,col\n\nstd_payment = payment_df.withColumn('payment_type',when(col('payment_type')=='credit_card','Credit Card') \\\n                                   .when(col('payment_type')=='boleto','Boleto') \\\n                                   .when(col('payment_type')=='debit_card','Debit Card') \\\n                                    .otherwise('Other'))\n\nstd_payment.select('payment_type').distinct().orderBy(length('payment_type')).show()"}, {"cell_type": "code", "execution_count": 35, "id": "b5278e31-3e0b-497b-8427-465ee787e3e1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+------------+--------------------+-------------+------+\n|            order_id|payment_sequential|payment_type|payment_installments|payment_value|Amount|\n+--------------------+------------------+------------+--------------------+-------------+------+\n|b81ef226f3fe1789b...|                 1| Credit Card|                   8|        99.33|    99|\n|a9810da82917af2d9...|                 1| Credit Card|                   1|        24.39|    24|\n|25e8ea4e93396b6fa...|                 1| Credit Card|                   1|        65.71|    65|\n|ba78997921bbcdc13...|                 1| Credit Card|                   8|       107.78|   107|\n|42fdf880ba16b47b5...|                 1| Credit Card|                   2|       128.45|   128|\n+--------------------+------------------+------------+--------------------+-------------+------+\nonly showing top 5 rows\n\n"}], "source": "std_payment = std_payment.withColumn('Amount', col('payment_value').cast('int'))\nstd_payment.show(5)"}, {"cell_type": "code", "execution_count": 9, "id": "7a96656b-852a-40f4-96e2-389383e94caa", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Schema of : Orders\nroot\n |-- order_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- order_status: string (nullable = true)\n |-- order_purchase_timestamp: timestamp (nullable = true)\n |-- order_approved_at: timestamp (nullable = true)\n |-- order_delivered_carrier_date: timestamp (nullable = true)\n |-- order_delivered_customer_date: timestamp (nullable = true)\n |-- order_estimated_delivery_date: timestamp (nullable = true)\n\n"}], "source": "print_schema(order_df,'Orders')"}, {"cell_type": "code", "execution_count": 10, "id": "2bf8cf06-ffd3-4c38-bbe2-1590e48b60ac", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n|            order_id|         customer_id|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|\n+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n|e481f51cbdc54678b...|9ef432eb625129730...|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|\n|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|   delivered|     2018-07-24 20:41:37|2018-07-26 03:24:27|         2018-07-26 14:31:00|          2018-08-07 15:27:45|          2018-08-13 00:00:00|\n|47770eb9100c2d0c4...|41ce2a54c0b03bf34...|   delivered|     2018-08-08 08:38:49|2018-08-08 08:55:23|         2018-08-08 13:50:00|          2018-08-17 18:06:29|          2018-09-04 00:00:00|\n|949d5b44dbf5de918...|f88197465ea7920ad...|   delivered|     2017-11-18 19:28:06|2017-11-18 19:45:59|         2017-11-22 13:39:59|          2017-12-02 00:28:42|          2017-12-15 00:00:00|\n|ad21c59c0840e6cb8...|8ab97904e6daea886...|   delivered|     2018-02-13 21:18:39|2018-02-13 22:20:29|         2018-02-14 19:46:34|          2018-02-16 18:17:02|          2018-02-26 00:00:00|\n+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\nonly showing top 5 rows\n\n"}], "source": "order_df.show(5)"}, {"cell_type": "code", "execution_count": 19, "id": "c93346d3-a6ed-460e-8e29-4c3b3a9b6724", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+\n|            order_id|         customer_id|order_status|order_purchase_timestamp|order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|\n+--------------------+--------------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+\n|e481f51cbdc54678b...|9ef432eb625129730...|   delivered|     2017-10-02 10:56:33|       2017-10-02|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|\n|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|   delivered|     2018-07-24 20:41:37|       2018-07-26|         2018-07-26 14:31:00|          2018-08-07 15:27:45|          2018-08-13 00:00:00|\n|47770eb9100c2d0c4...|41ce2a54c0b03bf34...|   delivered|     2018-08-08 08:38:49|       2018-08-08|         2018-08-08 13:50:00|          2018-08-17 18:06:29|          2018-09-04 00:00:00|\n|949d5b44dbf5de918...|f88197465ea7920ad...|   delivered|     2017-11-18 19:28:06|       2017-11-18|         2017-11-22 13:39:59|          2017-12-02 00:28:42|          2017-12-15 00:00:00|\n|ad21c59c0840e6cb8...|8ab97904e6daea886...|   delivered|     2018-02-13 21:18:39|       2018-02-13|         2018-02-14 19:46:34|          2018-02-16 18:17:02|          2018-02-26 00:00:00|\n+--------------------+--------------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+\nonly showing top 5 rows\n\n"}], "source": "from pyspark.sql.functions import to_date,col\n\ncleaned_order = order_df.withColumn('order_approved_at',to_date(col('order_approved_at')))\ncleaned_order.show(5)"}, {"cell_type": "markdown", "id": "a2b1b47a-2cad-4faa-b4dd-f6986509db73", "metadata": {}, "source": "## DeDuplication"}, {"cell_type": "code", "execution_count": 36, "id": "14265a6f-c771-4ae5-8c4b-154e11349808", "metadata": {}, "outputs": [], "source": "# if Primary key column on main columns itself duplicate , then Drop Duplicates\n\ncustomer_cleaned = customer_df.dropDuplicates(['customer_id']) "}, {"cell_type": "markdown", "id": "004d3b18-7dab-4be2-97e0-06d6b93bebbe", "metadata": {}, "source": "## join"}, {"cell_type": "code", "execution_count": 23, "id": "f36901a4-eb64-4e06-b6d9-7347342ecfec", "metadata": {}, "outputs": [], "source": "order_join = order_df.join(customer_df,'customer_id','left') \\\n                     .join(payment_df,'order_id','left')\\\n                     .join(item_df,'order_id','left')"}, {"cell_type": "code", "execution_count": 24, "id": "1ae4ec0b-f9a3-4ad6-86c2-c268652f1cc8", "metadata": {}, "outputs": [{"data": {"text/plain": "['order_id',\n 'customer_id',\n 'order_status',\n 'order_purchase_timestamp',\n 'order_approved_at',\n 'order_delivered_carrier_date',\n 'order_delivered_customer_date',\n 'order_estimated_delivery_date',\n 'customer_unique_id',\n 'customer_zip_code_prefix',\n 'customer_city',\n 'customer_state',\n 'payment_sequential',\n 'payment_type',\n 'payment_installments',\n 'payment_value',\n 'order_item_id',\n 'product_id',\n 'seller_id',\n 'shipping_limit_date',\n 'price',\n 'freight_value']"}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": "order_join.columns"}, {"cell_type": "code", "execution_count": 26, "id": "00f49e63-0a63-49f6-a9c7-9f653f0afb85", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 48:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----+------------+\n|         customer_id|price|payment_type|\n+--------------------+-----+------------+\n|9ef432eb625129730...|29.99|     voucher|\n|9ef432eb625129730...|29.99|     voucher|\n|9ef432eb625129730...|29.99| credit_card|\n|b0830fb4747a6c6d2...|118.7|      boleto|\n|41ce2a54c0b03bf34...|159.9| credit_card|\n+--------------------+-----+------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "order_join.select('customer_id','price','payment_type').show(5)"}, {"cell_type": "code", "execution_count": 30, "id": "5dd3db33-5a4d-473b-bbb2-d0e7a280d9b4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 66:===========================================>              (3 + 1) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----------+\n|         customer_id|total_price|\n+--------------------+-----------+\n|1617b1357756262bf...|    13440.0|\n|9af2372a1e4934027...|    11384.0|\n|de832e8dbb1f588a4...|    10856.0|\n|63b964e79dee32a35...|     9888.0|\n|6f241d5bbb142b6f7...|     9520.0|\n+--------------------+-----------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# top 5 Customers\nfrom pyspark.sql.functions import sum, round\n\norder_join.groupBy('customer_id').agg(round(sum('price')).alias('total_price')).orderBy('total_price',ascending=False).show(5)"}, {"cell_type": "code", "execution_count": null, "id": "566960f4-85fc-4c69-a0e8-8c8e364a8bca", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 44:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----------------+\n|            order_id|       sum(price)|\n+--------------------+-----------------+\n|f373335aac9a659de...|             35.9|\n|118045506e1c1dda0...|            225.0|\n|cc66dee6fbc18bb79...|            117.7|\n|f44cb69655f8e4d13...|           311.94|\n|edcc6b79e8394346b...|             99.9|\n|9f98d6530155e3b38...|            299.9|\n|5e57ff5e1c008db89...|           139.98|\n|0957ed870116e596b...|           129.99|\n|3fa59277573f0fe06...|             79.0|\n|d5f812041d8fc446c...|             64.9|\n|24012690fe6562f4a...|           129.99|\n|85be7c94bcd3f908f...|            59.99|\n|56ef80c564f6fd57c...|             34.9|\n|7a70b827ebc6ab85b...|535.6999999999999|\n|8e10a1d1a57b6a469...|           239.99|\n|107478e48c13dc0b3...|             71.3|\n|949280c70c6d62ec9...|             34.9|\n|6a276c227b7bb9659...|            179.9|\n|0fe9c7ad9288ff24b...|             34.9|\n|03ebfa9712b7dbc70...|             46.9|\n+--------------------+-----------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import *\n\ngp_df = order_join.groupBy('order_id').agg(sum('price')).show()"}, {"cell_type": "markdown", "id": "c5551150-81bf-4676-be4b-fe4b6d795ed5", "metadata": {}, "source": "## Calculating Top Sellers Based on Revenue Generated"}, {"cell_type": "code", "execution_count": 10, "id": "5fca4a5f-08d5-4d95-be78-f885cf5baaf9", "metadata": {}, "outputs": [], "source": "rev_df = order_df.join(payment_df,'order_id','left')\\\n                .join(item_df,'order_id','left') \\\n                .join(seller_df,'seller_id','left')"}, {"cell_type": "code", "execution_count": 13, "id": "b36b112a-d6ea-472b-8f4b-b57fbaf6001b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+-------------+\n|           seller_id|            order_id|payment_value|\n+--------------------+--------------------+-------------+\n|3504c0cb71d7fa48d...|e481f51cbdc54678b...|        18.59|\n|3504c0cb71d7fa48d...|e481f51cbdc54678b...|          2.0|\n|3504c0cb71d7fa48d...|e481f51cbdc54678b...|        18.12|\n+--------------------+--------------------+-------------+\nonly showing top 3 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "25/06/12 16:07:28 ERROR TransportClient: Failed to send RPC RPC 8392487895230431987 to /10.128.0.7:52148: io.netty.channel.StacklessClosedChannelException\nio.netty.channel.StacklessClosedChannelException: null\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n25/06/12 16:07:28 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 55 from block manager BlockManagerId(2, my-cluster-w-1.us-central1-a.c.keen-truth-461516-a0.internal, 45651, None)\njava.io.IOException: Failed to send RPC RPC 8392487895230431987 to /10.128.0.7:52148: io.netty.channel.StacklessClosedChannelException\n\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369) ~[spark-network-common_2.12-3.3.2.jar:3.3.2]\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.77.Final.jar:4.1.77.Final]\n\tat java.lang.Thread.run(Thread.java:829) ~[?:?]\nCaused by: io.netty.channel.StacklessClosedChannelException\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.77.Final.jar:4.1.77.Final]\n"}], "source": "rev_df.select(\n    item_df['seller_id'], \n    item_df['order_id'],  \n    payment_df['payment_value']\n).show(3)"}, {"cell_type": "code", "execution_count": 14, "id": "021f5163-713c-4852-8574-a59399d772c0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- seller_id: string (nullable = true)\n |-- order_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- order_status: string (nullable = true)\n |-- order_purchase_timestamp: timestamp (nullable = true)\n |-- order_approved_at: timestamp (nullable = true)\n |-- order_delivered_carrier_date: timestamp (nullable = true)\n |-- order_delivered_customer_date: timestamp (nullable = true)\n |-- order_estimated_delivery_date: timestamp (nullable = true)\n |-- payment_sequential: integer (nullable = true)\n |-- payment_type: string (nullable = true)\n |-- payment_installments: integer (nullable = true)\n |-- payment_value: double (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- product_id: string (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n |-- seller_zip_code_prefix: integer (nullable = true)\n |-- seller_city: string (nullable = true)\n |-- seller_state: string (nullable = true)\n\n"}], "source": "rev_df.printSchema()"}, {"cell_type": "code", "execution_count": 22, "id": "b81ff8e8-7ce5-4bc6-bc05-063c50df8a8f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 40:=============================>                            (2 + 2) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+\n|           seller_id|total_revenue|\n+--------------------+-------------+\n|7c67e1448b00f6e96...|     507167.0|\n|1025f0e2d44d7041d...|     308222.0|\n|4a3ca9315b744ce9f...|     301245.0|\n|1f50f920176fa81da...|     290253.0|\n|53243585a1d6dc264...|     284903.0|\n+--------------------+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import *\n\nrev_df.groupBy('seller_id') \\\n      .agg(round(sum('payment_value')).alias('total_revenue')) \\\n      .orderBy('total_revenue', ascending=False) \\\n      .show(5)"}, {"cell_type": "code", "execution_count": 41, "id": "754b099b-8cca-45ff-84f4-5eedb771a3e2", "metadata": {}, "outputs": [], "source": "!hadoop fs -mkdir /data/olist_join"}, {"cell_type": "code", "execution_count": 42, "id": "ba6bbb19-59be-4211-a6a0-557a25697575", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\ndrwxr-xr-x   - dgclub21 hadoop          0 2025-06-07 12:21 /data/olist\ndrwxr-xr-x   - root     hadoop          0 2025-06-12 16:28 /data/olist_join\n"}], "source": "!hadoop fs -ls /data/"}, {"cell_type": "code", "execution_count": 43, "id": "80902ce0-913c-4d9f-b5ae-72fac886cbb7", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "rev_df.write.mode('overwrite').parquet('/data/olist_join/join_df.parquet')"}, {"cell_type": "code", "execution_count": 45, "id": "8015cd98-3570-4d74-ae9d-d8911ea4e180", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\ndrwxr-xr-x   - root hadoop          0 2025-06-12 16:29 /data/olist_join/join_df.parquet\n"}], "source": "!hadoop fs -ls /data/olist_join"}, {"cell_type": "code", "execution_count": 46, "id": "e81ef233-7870-48fb-8d0e-11a10b497040", "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "markdown", "id": "fa43fa82-4344-4acc-9459-a16caedc0991", "metadata": {}, "source": "# Data Integration"}, {"cell_type": "code", "execution_count": 1, "id": "c1bc9e56-b989-43d2-90f9-82364e96f9af", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/14 08:09:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n        .appName('Data_Integeration') \\\n        .getOrCreate()"}, {"cell_type": "markdown", "id": "d645efff-0596-4d0d-8578-7e6f84405d99", "metadata": {}, "source": "## Caching"}, {"cell_type": "code", "execution_count": 65, "id": "5b6d5349-5cf3-4b71-b954-2349278d82fd", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[order_id: string, order_item_id: int, product_id: string, seller_id: string, shipping_limit_date: timestamp, price: double, freight_value: double]"}, "execution_count": 65, "metadata": {}, "output_type": "execute_result"}], "source": "order_df.cache()\ncustomer_df.cache()\nitem_df.cache()"}, {"cell_type": "markdown", "id": "db345c0c-3073-4358-919d-2fb99134cf11", "metadata": {}, "source": "## Joining all Tables"}, {"cell_type": "code", "execution_count": 28, "id": "12c24369-b467-4d2b-b011-6935ca807c0c", "metadata": {}, "outputs": [], "source": "order_item_join = order_df.join(item_df,'order_id','inner')"}, {"cell_type": "code", "execution_count": 29, "id": "83a1fc8a-2567-4b4e-b7f4-e7c1e213be44", "metadata": {}, "outputs": [], "source": "order_item_product = order_item_join.join(product_df,'product_id','inner')"}, {"cell_type": "code", "execution_count": 30, "id": "ef4a6bcb-d56e-4517-9576-b656879d97b7", "metadata": {}, "outputs": [], "source": "order_item_product_seller = order_item_product.join(seller_df,'seller_id','inner')"}, {"cell_type": "code", "execution_count": 32, "id": "54722c17-a74b-473c-bbeb-10b8796a1fd0", "metadata": {}, "outputs": [], "source": "full_order = order_item_product_seller.join(customer_df,'customer_id','inner')"}, {"cell_type": "code", "execution_count": 35, "id": "fdf8fa89-dd25-41a7-a23d-59b158a6ebc3", "metadata": {}, "outputs": [], "source": "# GeoLaction : location is not Important Factor, so inner join skips all details which misses location , for that now doing left join\n\nfull_order = full_order.join(\n    location_df,\n    full_order.customer_zip_code_prefix == location_df.geolocation_zip_code_prefix,\n    \"left\"\n)"}, {"cell_type": "code", "execution_count": 38, "id": "65f22970-7a6f-4501-8fe2-52a682812eb7", "metadata": {}, "outputs": [], "source": "full_order = full_order.join(review_df,'order_id','left')"}, {"cell_type": "code", "execution_count": 40, "id": "21a6331a-6c2f-4aae-a22e-174774423e4a", "metadata": {}, "outputs": [], "source": "full_order = full_order.join(payment_df,'order_id','left')"}, {"cell_type": "code", "execution_count": 41, "id": "09278e8a-16bd-4fc4-a016-956621a5914f", "metadata": {}, "outputs": [{"data": {"text/plain": "['order_id',\n 'customer_id',\n 'seller_id',\n 'product_id',\n 'order_status',\n 'order_purchase_timestamp',\n 'order_approved_at',\n 'order_delivered_carrier_date',\n 'order_delivered_customer_date',\n 'order_estimated_delivery_date',\n 'order_item_id',\n 'shipping_limit_date',\n 'price',\n 'freight_value',\n 'product_category_name',\n 'product_name_lenght',\n 'product_description_lenght',\n 'product_photos_qty',\n 'product_weight_g',\n 'product_length_cm',\n 'product_height_cm',\n 'product_width_cm',\n 'seller_zip_code_prefix',\n 'seller_city',\n 'seller_state',\n 'customer_unique_id',\n 'customer_zip_code_prefix',\n 'customer_city',\n 'customer_state',\n 'geolocation_zip_code_prefix',\n 'geolocation_lat',\n 'geolocation_lng',\n 'geolocation_city',\n 'geolocation_state',\n 'review_id',\n 'review_score',\n 'review_comment_title',\n 'review_comment_message',\n 'review_creation_date',\n 'review_answer_timestamp',\n 'payment_sequential',\n 'payment_type',\n 'payment_installments',\n 'payment_value']"}, "execution_count": 41, "metadata": {}, "output_type": "execute_result"}], "source": "full_order.columns"}, {"cell_type": "code", "execution_count": 70, "id": "e88ec60c-9b8b-4730-a4aa-13f934f3bb7c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- order_status: string (nullable = true)\n |-- order_purchase_timestamp: timestamp (nullable = true)\n |-- order_approved_at: timestamp (nullable = true)\n |-- order_delivered_carrier_date: timestamp (nullable = true)\n |-- order_delivered_customer_date: timestamp (nullable = true)\n |-- order_estimated_delivery_date: timestamp (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n |-- product_category_name: string (nullable = true)\n |-- product_name_lenght: integer (nullable = true)\n |-- product_description_lenght: integer (nullable = true)\n |-- product_photos_qty: integer (nullable = true)\n |-- product_weight_g: integer (nullable = true)\n |-- product_length_cm: integer (nullable = true)\n |-- product_height_cm: integer (nullable = true)\n |-- product_width_cm: integer (nullable = true)\n |-- seller_zip_code_prefix: integer (nullable = true)\n |-- seller_city: string (nullable = true)\n |-- seller_state: string (nullable = true)\n |-- customer_unique_id: string (nullable = true)\n |-- customer_zip_code_prefix: integer (nullable = true)\n |-- customer_city: string (nullable = true)\n |-- customer_state: string (nullable = true)\n |-- geolocation_zip_code_prefix: integer (nullable = true)\n |-- geolocation_lat: double (nullable = true)\n |-- geolocation_lng: double (nullable = true)\n |-- geolocation_city: string (nullable = true)\n |-- geolocation_state: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- review_score: string (nullable = true)\n |-- review_comment_title: string (nullable = true)\n |-- review_comment_message: string (nullable = true)\n |-- review_creation_date: string (nullable = true)\n |-- review_answer_timestamp: string (nullable = true)\n |-- payment_sequential: integer (nullable = true)\n |-- payment_type: string (nullable = true)\n |-- payment_installments: integer (nullable = true)\n |-- payment_value: double (nullable = true)\n\n"}], "source": "full_order.printSchema()"}, {"cell_type": "code", "execution_count": 42, "id": "cefc8766-5ae5-4d1b-ab18-f751afa33b4a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/14 09:30:33 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"}, {"data": {"text/plain": "DataFrame[order_id: string, customer_id: string, seller_id: string, product_id: string, order_status: string, order_purchase_timestamp: timestamp, order_approved_at: timestamp, order_delivered_carrier_date: timestamp, order_delivered_customer_date: timestamp, order_estimated_delivery_date: timestamp, order_item_id: int, shipping_limit_date: timestamp, price: double, freight_value: double, product_category_name: string, product_name_lenght: int, product_description_lenght: int, product_photos_qty: int, product_weight_g: int, product_length_cm: int, product_height_cm: int, product_width_cm: int, seller_zip_code_prefix: int, seller_city: string, seller_state: string, customer_unique_id: string, customer_zip_code_prefix: int, customer_city: string, customer_state: string, geolocation_zip_code_prefix: int, geolocation_lat: double, geolocation_lng: double, geolocation_city: string, geolocation_state: string, review_id: string, review_score: string, review_comment_title: string, review_comment_message: string, review_creation_date: string, review_answer_timestamp: string, payment_sequential: int, payment_type: string, payment_installments: int, payment_value: double]"}, "execution_count": 42, "metadata": {}, "output_type": "execute_result"}], "source": "full_order.cache()"}, {"cell_type": "markdown", "id": "7b1fe92c-9d1b-4cc7-b092-3e424c021175", "metadata": {}, "source": "# Optimized Join"}, {"cell_type": "markdown", "id": "9296d1cf-6147-44ad-9c8a-f3444555ae26", "metadata": {}, "source": "A broadcast join is used when one of your DataFrames is small enough to fit in memory. Spark broadcasts the smaller DataFrame to all worker nodes, which avoids shuffling the larger DataFrame across the cluster \u2014 making the join much faster."}, {"cell_type": "code", "execution_count": 66, "id": "4dea709f-b30d-4f70-8533-dbe47badc4bf", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import *"}, {"cell_type": "code", "execution_count": 67, "id": "bcfc8bfc-5962-4eaf-a9f3-09de8a5f45cb", "metadata": {}, "outputs": [], "source": "order_item_join = order_df.join(item_df,'order_id','inner')"}, {"cell_type": "code", "execution_count": 68, "id": "2ad0ec44-d4d1-4e62-84c0-f1c1838c1616", "metadata": {}, "outputs": [], "source": "order_item_product = order_item_join.join(product_df,'product_id','inner')"}, {"cell_type": "code", "execution_count": 69, "id": "3a1d5cc1-ab06-4c98-9c68-7a2c1fac6beb", "metadata": {}, "outputs": [], "source": "order_item_product_seller = order_item_product.join(broadcast(seller_df),'seller_id','inner')"}, {"cell_type": "code", "execution_count": 70, "id": "58716ecc-1215-4b4f-b004-50ff19016638", "metadata": {}, "outputs": [], "source": "full_order = order_item_product_seller.join(customer_df,'customer_id','inner')"}, {"cell_type": "code", "execution_count": 71, "id": "e9199003-415d-4984-a0fb-d03e59308b63", "metadata": {}, "outputs": [], "source": "# GeoLaction : location is not Important Factor, so inner join skips all details which misses location , for that now doing left join\n\nfull_order = full_order.join(\n    broadcast(location_df),\n    full_order.customer_zip_code_prefix == location_df.geolocation_zip_code_prefix,\n    \"left\"\n)"}, {"cell_type": "code", "execution_count": 72, "id": "f6d2da36-e8f5-40c0-a4b9-20681cb44d7a", "metadata": {}, "outputs": [], "source": "full_order = full_order.join(broadcast(review_df),'order_id','left')"}, {"cell_type": "code", "execution_count": 73, "id": "038b1e14-6159-47ba-9870-b11fa723dd20", "metadata": {}, "outputs": [], "source": "full_order = full_order.join(payment_df,'order_id','left')"}, {"cell_type": "code", "execution_count": 88, "id": "2950aaae-e8cb-43b7-a427-58ee5dc13f55", "metadata": {}, "outputs": [], "source": "full_order = full_order.join(broadcast(tc.select('customer_id','customer_segment')),'customer_id','left')"}, {"cell_type": "code", "execution_count": 98, "id": "3a4f6128-e5be-4dd4-9f65-59d4c934491b", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[customer_id: string, order_id: string, seller_id: string, product_id: string, order_status: string, order_purchase_timestamp: timestamp, order_approved_at: timestamp, order_delivered_carrier_date: timestamp, order_delivered_customer_date: timestamp, order_estimated_delivery_date: timestamp, order_item_id: int, shipping_limit_date: timestamp, price: double, freight_value: double, product_category_name: string, product_name_lenght: int, product_description_lenght: int, seller_zip_code_prefix: int, seller_city: string, seller_state: string, customer_unique_id: string, customer_zip_code_prefix: int, customer_city: string, customer_state: string, geolocation_zip_code_prefix: int, geolocation_lat: double, geolocation_lng: double, geolocation_city: string, geolocation_state: string, review_id: string, review_score: string, review_comment_title: string, review_comment_message: string, review_creation_date: string, review_answer_timestamp: string, payment_sequential: int, payment_type: string, payment_installments: int, payment_value: double, total_spent: double, total_orders: bigint, AOV: double, day_type: string, customer_segment: string]"}, "execution_count": 98, "metadata": {}, "output_type": "execute_result"}], "source": "full_order.cache()"}, {"cell_type": "markdown", "id": "52add69f-49b2-4b6d-b666-fe9a701db285", "metadata": {}, "source": "## Finding Duplicate Columns and Remove"}, {"cell_type": "code", "execution_count": 83, "id": "e62a92b0-8260-40de-8cc8-1b09c438bce7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Duplicate columns: ['customer_segment']\n"}], "source": "from collections import Counter\n\n# Get all column names\ncolumns = full_order.columns\n\n# Count occurrences\ncolumn_counts = Counter(columns)\n\n# Print duplicates\nduplicates = [col for col, count in column_counts.items() if count > 1]\nprint(\"Duplicate columns:\", duplicates)\n"}, {"cell_type": "code", "execution_count": 126, "id": "fc844d15-e1b2-4d32-b8f4-015c49ddcae5", "metadata": {}, "outputs": [], "source": "full_order = full_order.drop('geolocation_state','geolocation_zip_code_prefix','geolocation_city')"}, {"cell_type": "code", "execution_count": 122, "id": "acc459c3-67d7-4a20-baaf-ae2edec7a049", "metadata": {}, "outputs": [{"data": {"text/plain": "['customer_id',\n 'order_id',\n 'seller_id',\n 'product_id',\n 'order_status',\n 'order_purchase_timestamp',\n 'order_delivered_customer_date',\n 'order_estimated_delivery_date',\n 'order_item_id',\n 'shipping_limit_date',\n 'price',\n 'freight_value',\n 'seller_zip_code_prefix',\n 'seller_city',\n 'seller_state',\n 'customer_unique_id',\n 'customer_zip_code_prefix',\n 'customer_city',\n 'customer_state',\n 'geolocation_zip_code_prefix',\n 'geolocation_city',\n 'geolocation_state',\n 'review_score',\n 'payment_type',\n 'payment_value',\n 'total_spent',\n 'total_orders',\n 'AOV',\n 'day_type',\n 'customer_segment']"}, "execution_count": 122, "metadata": {}, "output_type": "execute_result"}], "source": "full_order.columns"}, {"cell_type": "code", "execution_count": 129, "id": "6d38342d-a7d6-41a4-8fe1-e86d4a270e48", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 159:============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "-RECORD 0----------------------------\n customer_id                   | 0   \n order_id                      | 0   \n seller_id                     | 0   \n product_id                    | 0   \n order_status                  | 0   \n order_purchase_timestamp      | 0   \n order_delivered_customer_date | 0   \n order_estimated_delivery_date | 0   \n order_item_id                 | 0   \n shipping_limit_date           | 0   \n price                         | 0   \n freight_value                 | 0   \n seller_zip_code_prefix        | 0   \n seller_city                   | 0   \n seller_state                  | 0   \n customer_unique_id            | 0   \n customer_zip_code_prefix      | 0   \n customer_city                 | 0   \n customer_state                | 0   \n review_score                  | 0   \n payment_type                  | 0   \n payment_value                 | 0   \n total_spent                   | 0   \n total_orders                  | 0   \n AOV                           | 0   \n day_type                      | 0   \n customer_segment              | 0   \n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import col, when, count\n\nfull_order.select([\n    count(when(col(c).isNull(), 1)).alias(c) for c in full_order.columns\n]).show(vertical=True)"}, {"cell_type": "markdown", "id": "299cc39a-4652-48e7-811f-ce580f57e20b", "metadata": {}, "source": "## Filling Mean Values for Null"}, {"cell_type": "code", "execution_count": 115, "id": "45e4c8f5-06b0-4459-9c55-d2d0f9cf5114", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "mean_value = full_order.select(mean('payment_value')).collect()[0][0]\nfull_order = full_order.fillna({'payment_value':mean_value})"}, {"cell_type": "code", "execution_count": 123, "id": "63403a26-8b40-4f71-9b86-d45f8acec518", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "review_mean = full_order.select(mean('review_score')).collect()[0][0]\nfull_order = full_order.fillna({'review_score':review_mean})"}, {"cell_type": "code", "execution_count": 128, "id": "389eb7d0-f4e1-41d8-887f-894c0e4266de", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import when, col\n\nfull_order = full_order.withColumn(\n    'order_delivered_customer_date',\n    when(\n        col('order_delivered_customer_date').isNull(),\n        col('order_estimated_delivery_date')\n    ).otherwise(col('order_delivered_customer_date'))\n)"}, {"cell_type": "code", "execution_count": 116, "id": "a4eea476-76af-4f1c-9489-c220213c93e6", "metadata": {}, "outputs": [], "source": "full_order = full_order.fillna({'payment_type':'Cash on Delivery'})"}, {"cell_type": "markdown", "id": "6a1ab35d-9305-49fa-bf52-24e9c536e1df", "metadata": {}, "source": "# Analysis & Aggregation"}, {"cell_type": "markdown", "id": "b09e52b8-68f6-441e-b014-fbba050f4f39", "metadata": {}, "source": "## Total Revenue per Seller"}, {"cell_type": "code", "execution_count": 13, "id": "a134330c-33db-48b7-a735-9b4d46d78374", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 25:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+\n|           seller_id|             revenue|\n+--------------------+--------------------+\n|4869f7a5dfa277a7d...|3.6138717319998816E7|\n|53243585a1d6dc264...| 3.429159295000016E7|\n|4a3ca9315b744ce9f...| 3.375957084003399E7|\n|7c67e1448b00f6e96...|3.2282321790014144E7|\n|fa1c13f2614d7b5c4...| 3.013938631000357E7|\n|da8622b14eb17ae28...|  2.98576697300434E7|\n|7e93a43ef30c4f03f...| 2.631570630000493E7|\n|1025f0e2d44d7041d...|2.2937518520012498E7|\n|46dc3b2cc0980fb8e...| 2.179177329001596E7|\n|955fee9216a65b617...|2.0964410670014285E7|\n+--------------------+--------------------+\nonly showing top 10 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import sum\n\nfull_order.groupBy('seller_id') \\\n    .agg(sum('price').alias('revenue')) \\\n    .orderBy('revenue', ascending=False) \\\n    .show(10)"}, {"cell_type": "code", "execution_count": 71, "id": "dd7a8341-15d9-4cd9-989c-b83ab8d3bcf9", "metadata": {}, "outputs": [], "source": "seller_revenue = full_order.groupBy('seller_id') \\\n    .agg(round(sum('price')).alias('revenue')) \\\n    .orderBy('revenue', ascending=False)"}, {"cell_type": "code", "execution_count": 72, "id": "e58535f8-6ec9-4cf7-80a7-cfa8a2f5b423", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 142:====================================================>(198 + 1) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----------+\n|           seller_id|    revenue|\n+--------------------+-----------+\n|4869f7a5dfa277a7d...|3.6138717E7|\n|53243585a1d6dc264...|3.4291593E7|\n|4a3ca9315b744ce9f...|3.3759571E7|\n|7c67e1448b00f6e96...|3.2282322E7|\n|fa1c13f2614d7b5c4...|3.0139386E7|\n|da8622b14eb17ae28...| 2.985767E7|\n|7e93a43ef30c4f03f...|2.6315706E7|\n|1025f0e2d44d7041d...|2.2937519E7|\n|46dc3b2cc0980fb8e...|2.1791773E7|\n|955fee9216a65b617...|2.0964411E7|\n+--------------------+-----------+\nonly showing top 10 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "seller_revenue.show(10)"}, {"cell_type": "markdown", "id": "f41d8c50-0a3e-4714-add9-96292515d44e", "metadata": {}, "source": "## Total Orders per Customers\n"}, {"cell_type": "code", "execution_count": 55, "id": "1c48802a-dd54-47e7-8c65-f31cdf6fb190", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 68:=====================================================>(198 + 1) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------------+\n|         customer_id|total_orders|\n+--------------------+------------+\n|351e40989da90e704...|       11427|\n|50920f8cd0681fd86...|       10752|\n|9b43e2a62de9bab3a...|        8556|\n|270c23a11d024a44c...|        8001|\n|5c87184371002d49e...|        6876|\n+--------------------+------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "toc = full_order.groupBy('customer_id') \\\n        .agg(count('order_id').alias('total_orders')) \\\n        .orderBy('total_orders',ascending=False)\n\ntoc.show(5)"}, {"cell_type": "markdown", "id": "95602e34-2cd1-4ba3-b84b-127e8be822a6", "metadata": {}, "source": "## Average Review Score Per Seller"}, {"cell_type": "code", "execution_count": 56, "id": "3d544df8-6411-4d47-b0a8-2d8cf7959906", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 75:=====================================================>(198 + 1) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+----------------+\n|           seller_id|avg_review_score|\n+--------------------+----------------+\n|9c1c0c36cd23c2089...|             5.0|\n|8c351ed7c326c6212...|             5.0|\n|ec933281fb017b502...|             5.0|\n|f5b84683a9bf9e1df...|             5.0|\n|b2eecf5ea250510da...|             5.0|\n+--------------------+----------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "arc = full_order.groupBy('seller_id') \\\n        .agg(avg('review_score').alias('avg_review_score')) \\\n        .orderBy('avg_review_score',ascending=False)\n\narc.show(5)"}, {"cell_type": "markdown", "id": "4174c427-b4d5-450c-865c-88d2a3c5937a", "metadata": {}, "source": "## Top 10 Sold Products"}, {"cell_type": "code", "execution_count": 59, "id": "ec767ac7-6920-4bdd-aedd-83539a1c18e4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 96:=====================================================>(197 + 2) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+----------+\n|          product_id|total_sold|\n+--------------------+----------+\n|aca2eb7d00ea1a7b8...|     86740|\n|422879e10f4668299...|     81110|\n|99a4788cb24856965...|     78775|\n|389d119b48cf3043d...|     60248|\n|d1c427060a0f73f6b...|     59274|\n|368c6c730842d7801...|     58358|\n|53759a2ecddad2bb8...|     52654|\n|53b36df67ebb7c415...|     52105|\n|154e7e31ebfa09220...|     42700|\n|3dd2a17168ec895c7...|     40787|\n+--------------------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "tsp = full_order.groupBy('product_id') \\\n                .agg(count('order_id').alias('total_sold')) \\\n                .orderBy('total_sold',ascending=False) \\\n                .limit(10)\n\ntsp.show()"}, {"cell_type": "markdown", "id": "0e2d4b11-b44f-4277-9bba-b055e53d2367", "metadata": {}, "source": "## Top 10 Customers by Spending"}, {"cell_type": "code", "execution_count": 47, "id": "3212c43d-2b7b-401a-b00b-d364cde3a7a5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 62:==============>                                           (1 + 3) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+\n|         customer_id|total_payment|\n+--------------------+-------------+\n|d3e82ccec3cb5f956...|    6662844.0|\n|df55c14d1476a9a34...|    3565657.0|\n|fe5113a38e3575c04...|    3293604.0|\n|ec5b2ba62e5743423...|    2556120.0|\n|63b964e79dee32a35...|    2501664.0|\n|46bb3c0b1a65c8399...|    2336752.0|\n|05455dfa7cd02f13d...|    2160194.0|\n|3690e975641f01bd0...|    2124498.0|\n|349509b216bd5ec11...|    1923627.0|\n|695476b5848d64ba0...|    1820543.0|\n+--------------------+-------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "tcs = full_order.groupBy('customer_id') \\\n                .agg(round(sum('price')).alias('total_payment')) \\\n                .orderBy('total_payment',ascending=False) \\\n                .limit(10)\n\ntcs.show()"}, {"cell_type": "markdown", "id": "6d0753e8-9a1e-4270-a270-1d60d48d12ba", "metadata": {}, "source": "# Window Function & Ranking"}, {"cell_type": "code", "execution_count": 78, "id": "cd6f3b23-e8d8-4690-be74-1b875f482e50", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 178:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----+----+\n|           seller_id|price|Rank|\n+--------------------+-----+----+\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n+--------------------+-----+----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Ranking Top Selling Products Per Customer\n\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy('seller_id').orderBy(col('price').desc())\n\ntop_seller_products = full_order.withColumn('Rank', rank().over(window_spec)).filter(col('Rank')<=5)\ntop_seller_products.select('seller_id','price','Rank').show()"}, {"cell_type": "code", "execution_count": 87, "id": "dbae1c7c-463d-4cd1-b033-5c56c05281b4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 228:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----+----+\n|           seller_id|price|rank|\n+--------------------+-----+----+\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n|0015a82c2db000af6...|895.0|   1|\n+--------------------+-----+----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.window import Window\n\ntsp = full_order.withColumn('rank', rank().over(Window.partitionBy('seller_id').orderBy(col('price').desc()))) \\\n                .filter(col('rank')<=5) \n\ntsp.select('seller_id','price','rank').show(5)"}, {"cell_type": "markdown", "id": "bc59765c-6bd7-43e4-b2f9-eea867b9a2ad", "metadata": {}, "source": "# Advanced Aggregation & Enrichment"}, {"cell_type": "markdown", "id": "bf1133ba-8e90-4e05-a234-09bd18311b2b", "metadata": {}, "source": "## Total Revenue & Average Order Value (AOV) per Customer"}, {"cell_type": "code", "execution_count": 52, "id": "e53d027f-26e9-4dfd-9533-2f9316f7df38", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 64:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------------+-----------+------+\n|         customer_id|total_orders|total_spend|   AOV|\n+--------------------+------------+-----------+------+\n|d3e82ccec3cb5f956...|        6876|  6662844.0| 969.0|\n|df55c14d1476a9a34...|         743|  3565657.0|4799.0|\n|fe5113a38e3575c04...|        2292|  3293604.0|1437.0|\n|ec5b2ba62e5743423...|        1428|  2556120.0|1790.0|\n|63b964e79dee32a35...|        6072|  2501664.0| 412.0|\n|46bb3c0b1a65c8399...|         748|  2336752.0|3124.0|\n|05455dfa7cd02f13d...|        2184|  2160194.0| 989.0|\n|3690e975641f01bd0...|         802|  2124498.0|2649.0|\n|349509b216bd5ec11...|         743|  1923627.0|2589.0|\n|695476b5848d64ba0...|         687|  1820543.0|2650.0|\n|73236a0796f53d60d...|         832|  1755520.0|2110.0|\n|cc803a2c412833101...|         762|  1676400.0|2200.0|\n|1ff773612ab8934db...|        5820|  1658642.0| 285.0|\n|fced842c7dad61e8c...|         602|  1654898.0|2749.0|\n|1ecb47d23dc8203cd...|        1164|  1629588.0|1400.0|\n|de832e8dbb1f588a4...|        2190|  1584991.0| 724.0|\n|803cd9b04f9cd252c...|         488|  1512312.0|3099.0|\n|d72181923840c8895...|        2721|  1488115.0| 547.0|\n|06d478ba352a27a51...|        1146|  1461150.0|1275.0|\n|0049e8442c2a3e4a8...|        1204|  1444800.0|1200.0|\n+--------------------+------------+-----------+------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customer_spend = full_order.groupBy('customer_id')\\\n.agg(\n    count('order_id').alias('total_orders'),\n    round(sum('price')).alias('total_spend')\n)\\\n.withColumn('AOV',round(col('total_spend')/col('total_orders')))\\\n.orderBy(desc('total_spend'))\n\ncustomer_spend.show()"}, {"cell_type": "markdown", "id": "696e67ba-ac68-42e8-bcc5-45073520ea12", "metadata": {}, "source": "## Seller Performance Metrics ( Revenue , Average Reivew , Order  Count)"}, {"cell_type": "code", "execution_count": 27, "id": "f05fc558-fc8e-44f8-864d-b84f156095b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 37:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------------+-------------+------+-----------------+\n|           seller_id|total_orders|total_revenue|review|price_variability|\n+--------------------+------------+-------------+------+-----------------+\n|4869f7a5dfa277a7d...|      184587|  3.6138717E7|   4.0|           111.65|\n|53243585a1d6dc264...|       54514|  3.4291593E7|   4.0|           499.65|\n|4a3ca9315b744ce9f...|      330661|  3.3759571E7|   4.0|            59.37|\n|7c67e1448b00f6e96...|      233306|  3.2282322E7|   3.0|            50.39|\n|fa1c13f2614d7b5c4...|       87686|  3.0139386E7|   4.0|            307.7|\n|da8622b14eb17ae28...|      264433|   2.985767E7|   4.0|            72.92|\n|7e93a43ef30c4f03f...|       50226|  2.6315706E7|   4.0|           377.24|\n|1025f0e2d44d7041d...|      229587|  2.2937519E7|   4.0|             84.3|\n|46dc3b2cc0980fb8e...|       90426|  2.1791773E7|   4.0|           187.49|\n|955fee9216a65b617...|      232364|  2.0964411E7|   4.0|            84.94|\n|7a67c85e85bb2ce85...|      167231|  2.0312795E7|   4.0|            56.23|\n|620c87c171fb2a6dd...|      142232|   2.011984E7|   4.0|           100.45|\n|7d13fca1522535862...|       88807|  1.8156882E7|   4.0|           151.18|\n|a1043bafd471dff53...|      132672|  1.7662676E7|   4.0|            37.19|\n|6560211a19b47992c...|      286539|  1.7315933E7|   4.0|            35.04|\n|edb1ef5e36e0c8cd8...|       38945|  1.6624835E7|   4.0|           460.85|\n|1f50f920176fa81da...|      297292|  1.6497454E7|   4.0|             7.39|\n|5dceca129747e92ff...|       50420|  1.4910548E7|   4.0|           299.84|\n|cc419e0650a3c5ba7...|      256032|  1.4751465E7|   4.0|            22.67|\n|3d871de0142ce09b7...|      175876|  1.4184525E7|   4.0|            38.14|\n+--------------------+------------+-------------+------+-----------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "seller_performance = full_order.groupBy('seller_id')\\\n.agg(\n    count('order_id').alias('total_orders') ,\n    round(sum('price')).alias('total_revenue') ,\n    round(avg('review_score')).alias('review') ,\n    round(stddev('price'),2).alias('price_variability')\n)\\\n.orderBy('total_revenue','review',ascending=False)\n\nseller_performance.show()"}, {"cell_type": "markdown", "id": "4589d1f2-2460-4259-945f-36ec81413aee", "metadata": {}, "source": "## Product Popularity Metrics"}, {"cell_type": "code", "execution_count": 34, "id": "c7c2a8aa-f890-4182-beb8-59eb50fd2e0a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 43:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----------+-------------+-------------+--------------+--------------------+\n|          product_id|total_Sales|total_revenue|average_price|price_variable|      unique_sellers|\n+--------------------+-----------+-------------+-------------+--------------+--------------------+\n|aca2eb7d00ea1a7b8...|      86740|    6164630.0|         71.0|           3.0|[955fee9216a65b61...|\n|422879e10f4668299...|      81110|    4442792.0|         55.0|           4.0|[1f50f920176fa81d...|\n|99a4788cb24856965...|      78775|    6921763.0|         88.0|           4.0|[4a3ca9315b744ce9...|\n|389d119b48cf3043d...|      60248|    3280533.0|         54.0|           4.0|[1f50f920176fa81d...|\n|d1c427060a0f73f6b...|      59274|    8220103.0|        139.0|          17.0|[a1043bafd471dff5...|\n|368c6c730842d7801...|      58358|    3181699.0|         55.0|           5.0|[1f50f920176fa81d...|\n|53759a2ecddad2bb8...|      52654|    2893017.0|         55.0|           5.0|[1f50f920176fa81d...|\n|53b36df67ebb7c415...|      52105|    6159887.0|        118.0|          20.0|[7d13fca152253586...|\n|154e7e31ebfa09220...|      42700|     962161.0|         23.0|           2.0|[cc419e0650a3c5ba...|\n|3dd2a17168ec895c7...|      40787|    6116941.0|        150.0|           1.0|[de722cd6dad950a9...|\n|e53e557d5a159f5aa...|      39516|    3329354.0|         84.0|          11.0|[6973a06f484aacf4...|\n|2b4609f8948be1887...|      36179|    3171619.0|         88.0|           4.0|[cc419e0650a3c5ba...|\n|35afc973633aaeb6b...|      31206|    2735669.0|         88.0|           3.0|[d20b021d3efdf267...|\n|e0d64dcfaa3b6db5c...|      31153|    5226408.0|        168.0|          31.0|[7d13fca152253586...|\n|42a2c92a0979a949c...|      30486|    1810926.0|         59.0|           1.0|[813348c996469b40...|\n|7c1bd920dbdf22470...|      29018|    1739339.0|         60.0|           3.0|[cc419e0650a3c5ba...|\n|a62e25e09e05e6faf...|      28898|    3079869.0|        107.0|           1.0|[634964b17796e643...|\n|5a848e4ab52fd5445...|      28737|    3534364.0|        123.0|           0.0|[c826c40d7b19f62a...|\n|c4baedd846ed09b85...|      28166|    2802045.0|         99.0|          12.0|[a1043bafd471dff5...|\n|b532349fe46b38fbc...|      27176|     993090.0|         37.0|           2.0|[1025f0e2d44d7041...|\n+--------------------+-----------+-------------+-------------+--------------+--------------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import *\n\nppm = full_order.groupBy('product_id')\\\n.agg(\n    count('order_id').alias('total_Sales') ,\n    round(sum('price')).alias('total_revenue') ,\n    round(avg('price')).alias('average_price') , \n    round(stddev('price')).alias('price_variable') ,\n    collect_set('seller_id').alias('unique_sellers')\n)\\\n.orderBy(desc('total_sales'))\n    \nppm.show()"}, {"cell_type": "markdown", "id": "c8b8a1db-0393-434f-a485-3c19cf5d5419", "metadata": {}, "source": "## Customer Retention Analysis ( First & Last Order )"}, {"cell_type": "code", "execution_count": 38, "id": "d6190e9d-8d3b-41db-9c76-e104381a8c10", "metadata": {}, "outputs": [], "source": "customer_retention = full_order.groupBy('customer_id')\\\n.agg(\n    first('order_purchase_timestamp').alias('first_order') , \n    last('order_purchase_timestamp').alias('last_order') ,\n    count('order_id').alias('total_orders')\n)\\\n.withColumn('date_diff',datediff('first_order','last_order')) \\\n.orderBy(desc('date_diff'))"}, {"cell_type": "code", "execution_count": null, "id": "628110f5-b123-4243-938c-41352cd97b85", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 49:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+-------------------+------------+---------+\n|         customer_id|        first_order|         last_order|total_orders|date_diff|\n+--------------------+-------------------+-------------------+------------+---------+\n|74ad5dd7aac0a613b...|2017-03-28 14:32:09|2017-03-28 14:32:09|         101|        0|\n|6b0741b9cab6fa1a2...|2018-05-09 23:13:39|2018-05-09 23:13:39|         144|        0|\n|d713d3c4ca576a7fe...|2018-01-15 12:58:22|2018-01-15 12:58:22|         193|        0|\n|8915549cc658e933b...|2018-02-16 08:56:37|2018-02-16 08:56:37|          12|        0|\n|dc76dea012fc4ca16...|2018-06-11 10:11:47|2018-06-11 10:11:47|         169|        0|\n|15d1752086d06a721...|2018-05-05 13:36:10|2018-05-05 13:36:10|          32|        0|\n|6c4b7dd5c99515fe3...|2017-03-13 16:49:06|2017-03-13 16:49:06|         156|        0|\n|6e303063ecd7e907c...|2018-04-19 13:51:50|2018-04-19 13:51:50|         553|        0|\n|cd07153a3e03b42c9...|2018-03-05 14:47:54|2018-03-05 14:47:54|          65|        0|\n|4e6956b0bd38979ce...|2018-06-04 23:41:19|2018-06-04 23:41:19|          92|        0|\n|0c21938c6aa9cc9fc...|2017-10-04 13:18:43|2017-10-04 13:18:43|         487|        0|\n|3c3e0d4f138dc3ac5...|2018-01-24 09:38:40|2018-01-24 09:38:40|          24|        0|\n|8000019b04e81d5db...|2018-04-03 00:01:57|2018-04-03 00:01:57|          72|        0|\n|96f7db3d99ec562e1...|2017-12-14 23:24:00|2017-12-14 23:24:00|          23|        0|\n|5bc63bfa9fad97641...|2017-03-19 22:05:51|2017-03-19 22:05:51|          38|        0|\n|82e7f0a303085c904...|2017-06-15 14:12:15|2017-06-15 14:12:15|         108|        0|\n|e96c90e9a18c5fca8...|2017-09-19 12:20:28|2017-09-19 12:20:28|          47|        0|\n|bb447b65c8796bb95...|2018-01-15 17:24:30|2018-01-15 17:24:30|         151|        0|\n|24612642f0afb9ebb...|2017-12-29 16:55:21|2017-12-29 16:55:21|         139|        0|\n|d6b41b191f1f643ff...|2017-10-09 09:58:11|2017-10-09 09:58:11|          38|        0|\n+--------------------+-------------------+-------------------+------------+---------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customer_retention.show()"}, {"cell_type": "markdown", "id": "6092944b-2248-4ccd-8f45-5354da0e22fe", "metadata": {}, "source": "## Delivery Time Difference"}, {"cell_type": "code", "execution_count": 77, "id": "1b57a351-f4cd-44aa-85d3-c3e029b15cce", "metadata": {}, "outputs": [], "source": "delivery_time = full_order.groupBy('customer_id')\\\n.agg(\n    min('order_purchase_timestamp').alias('orderd_date') , \n    max('order_delivered_customer_date').alias('deliverd_date') ,\n    count('order_id').alias('total_orders')\n)\\\n.withColumn('date_diff',datediff('deliverd_date','orderd_date')) \\\n.orderBy(desc('date_diff'))"}, {"cell_type": "code", "execution_count": 46, "id": "31dc9fb6-ec1a-49b4-9083-e619c85c0d65", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 58:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+-------------------+------------+---------+\n|         customer_id|        orderd_date|      deliverd_date|total_orders|date_diff|\n+--------------------+-------------------+-------------------+------------+---------+\n|75683a92331068e2d...|2017-02-21 23:31:27|2017-09-19 14:36:39|          68|      210|\n|d306426abe5fca15e...|2018-02-23 14:57:35|2018-09-19 23:24:07|         159|      208|\n|7815125148cfa1e8c...|2017-03-07 23:59:51|2017-09-19 15:12:50|          45|      196|\n|217906bc11a32c1e4...|2017-03-08 18:09:02|2017-09-19 14:33:17|          59|      195|\n|9cf2c3fa2632cee74...|2017-03-08 22:47:40|2017-09-19 14:00:04|         112|      195|\n|1a8a4a30dc2969767...|2017-03-09 13:26:57|2017-09-19 14:38:21|           2|      194|\n|cb2caaaead400c973...|2018-01-03 09:44:01|2018-07-13 20:51:31|         144|      191|\n|65b14237885b3972e...|2017-03-13 20:17:10|2017-09-19 17:00:07|         155|      190|\n|8199345f57c6d1cbe...|2017-03-15 11:24:27|2017-09-19 14:38:18|         110|      188|\n|f85e9ec0719b16dc4...|2017-03-15 23:23:17|2017-09-19 17:14:25|          69|      188|\n|9b39de85d94d55a21...|2017-03-16 11:36:00|2017-09-19 16:28:58|         186|      187|\n|8f6ceed676a529b29...|2017-03-17 12:32:22|2017-09-19 18:13:19|         153|      186|\n|59b42de1617fdda0b...|2017-05-17 19:09:02|2017-11-16 10:56:45|         502|      183|\n|6852966131028b669...|2017-02-28 14:56:37|2017-08-28 16:23:46|         115|      181|\n|beeda72b31be3b8a3...|2017-06-12 13:14:11|2017-12-04 18:36:29|         840|      175|\n|bc34456d3b02a05a1...|2017-03-29 13:57:55|2017-09-19 15:07:09|          87|      174|\n|14fcb7be0e0a5868f...|2017-11-29 15:10:14|2018-05-21 18:22:18|          41|      173|\n|5c42637f3e7e7c520...|2017-03-31 15:03:51|2017-09-19 18:24:46|           6|      172|\n|bf391aa5a48fbaee5...|2018-02-02 21:38:36|2018-07-20 23:37:50|          12|      168|\n|5b316feca7e939c21...|2017-04-04 23:21:02|2017-09-19 13:47:09|         242|      168|\n+--------------------+-------------------+-------------------+------------+---------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "delivery_time.show()"}, {"cell_type": "code", "execution_count": 17, "id": "39aa1a38-9545-4bbb-8c6b-0a8a774f4f4e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 29:===========================================>              (3 + 1) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------+--------+\n|order_status|   count|\n+------------+--------+\n|     shipped|  165508|\n|    canceled|   81273|\n|    invoiced|   64213|\n|   delivered|17692295|\n|  processing|   59073|\n|    approved|     658|\n| unavailable|    1241|\n+------------+--------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_order.select('order_status').groupBy('order_status').count().show()"}, {"cell_type": "markdown", "id": "e69d558f-96f4-450a-badb-fade79dbd1d3", "metadata": {}, "source": "## SQL"}, {"cell_type": "code", "execution_count": 25, "id": "6a2cb1ac-915e-409e-ba87-53a529673552", "metadata": {}, "outputs": [], "source": "full_order.createTempView('dataset')"}, {"cell_type": "code", "execution_count": 26, "id": "36e71aff-ec75-4534-b258-1c43fe60ddb3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+\n|order_status|\n+------------+\n|   delivered|\n|   delivered|\n|   delivered|\n|   delivered|\n+------------+\nonly showing top 4 rows\n\n"}], "source": "spark.sql('select order_status from dataset').show(4)"}, {"cell_type": "code", "execution_count": 38, "id": "4ff70193-a3c7-450f-bae5-bc34a11cb585", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+------------+-----------+\n|order_status|total_count|\n+------------+-----------+\n|     shipped|     165508|\n|    canceled|      81273|\n|    invoiced|      64213|\n|   delivered|   17692295|\n|  processing|      59073|\n|    approved|        658|\n| unavailable|       1241|\n+------------+-----------+\n\n"}], "source": "spark.sql('''select order_status,\n                    count(order_status) as total_count\n                    from dataset\n                    group by order_status\n                    ''').show()"}, {"cell_type": "markdown", "id": "c46f68f6-277b-4b06-94c3-8f3289349ab7", "metadata": {}, "source": "## Order Status Flags"}, {"cell_type": "code", "execution_count": 39, "id": "8bdc6a40-9525-4385-9e10-db4ef525000e", "metadata": {}, "outputs": [], "source": "full_order = full_order.withColumn('is_delivered',when(col('order_status')=='delivered',lit(1)).otherwise(lit(0)))"}, {"cell_type": "code", "execution_count": 43, "id": "132a7bc6-1e82-4610-8318-730bc2a6f6ec", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 56:=============================>                            (2 + 2) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------+-------------------+\n|is_delivered|count(is_delivered)|\n+------------+-------------------+\n|           1|           17692295|\n|           0|             371966|\n+------------+-------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_order.select('order_status','is_delivered').groupBy('is_delivered').agg(count('is_delivered')).show(5)"}, {"cell_type": "code", "execution_count": 55, "id": "cb464bbe-553d-46bc-a240-1f6584b43015", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+-------------+-------------+\n|price|freight_value|order_revenue|\n+-----+-------------+-------------+\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n|29.99|         8.72|        38.71|\n+-----+-------------+-------------+\nonly showing top 20 rows\n\n"}], "source": "full_order = full_order.withColumn('order_revenue',col('price')+col('freight_value'))\nfull_order.select('price','freight_value','order_revenue').show()"}, {"cell_type": "markdown", "id": "927a21d1-fe90-4c7c-bc56-40f645a07bdd", "metadata": {}, "source": "## Customer Segmentation based on Spending"}, {"cell_type": "code", "execution_count": 53, "id": "bc9eada9-70ed-4758-ba82-9042d8fd59e1", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 67:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----------+------------+-----+\n|         customer_id|total_spent|total_orders|  AOV|\n+--------------------+-----------+------------+-----+\n|41ce2a54c0b03bf34...|     3998.0|          25|160.0|\n|f54a9f0e6b351c431...|       60.0|           3| 20.0|\n|2a1dfb647f32f4390...|    34710.0|          78|445.0|\n|4f28355e5c17a4a42...|    32387.0|         233|139.0|\n|4632eb5a8f175f6fe...|    21413.0|         268| 80.0|\n|57ee2ef64f17a5f9a...|    14286.0|         333| 43.0|\n|cc3590e4afbb4b3e0...|     3400.0|         272| 13.0|\n|843ff05b30ce4f75b...|     5518.0|          65| 85.0|\n|a4156bb8aff5d6722...|     2624.0|          15|175.0|\n|084dab2db2bf5d426...|     4751.0|          72| 66.0|\n|aa012a8928021b41e...|    10485.0|         150| 70.0|\n|a2ae1a05b2058776a...|     5944.0|         119| 50.0|\n|7089a41f6a02e9b57...|     1036.0|          28| 37.0|\n|1099d033c74a027a7...|     8683.0|         174| 50.0|\n|7a3bd3b37285f0ab2...|    50688.0|         512| 99.0|\n|c499f24d5aca03c90...|     1676.0|          58| 29.0|\n|6561ef6306e6369b4...|     1054.0|          31| 34.0|\n|ce8bb2a8b394a2e1f...|    14861.0|         186| 80.0|\n|690834a6694434cfb...|    24134.0|         161|150.0|\n|e1ab4ef68ece8b472...|     5837.0|         217| 27.0|\n+--------------------+-----------+------------+-----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# top customers \n\ntc = full_order.groupBy('customer_id')\\\n.agg(\n    round(sum('price')).alias('total_spent') ,\n    count('order_id').alias('total_orders')\n)\\\n.withColumn('AOV',round(col('total_spent')/col('total_orders')))\n\ntc.show()"}, {"cell_type": "code", "execution_count": 54, "id": "ccf12e01-e0cc-46af-8d13-94576dc3d352", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 70:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------+--------+------------------+\n|min(AOV)|max(AOV)|          avg(AOV)|\n+--------+--------+------------------+\n|     1.0|  6735.0|125.96647274643747|\n+--------+--------+------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import col, when\n\ntc = tc.withColumn(\n    'customer_segment',\n    when(col('AOV') >= 1200, 'High_Value')\n    .when(col('AOV').between(500, 1200), 'Mid_Value')  # Note: upper bound is exclusive\n    .otherwise('Low_Value')\n)\ntc.select(min('AOV'),max('AOV'),avg('AOV')).show()"}, {"cell_type": "code", "execution_count": 79, "id": "37567d70-cb41-4896-be43-cd68591b5d23", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 135:==========================================>              (3 + 1) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------------+-----+\n|customer_segment|count|\n+----------------+-----+\n|      High_Value|  596|\n|       Low_Value|95460|\n|       Mid_Value| 2610|\n+----------------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "tc.groupBy('customer_segment').count().show()\n"}, {"cell_type": "markdown", "id": "45412f67-e160-4c22-992f-8d2898953f51", "metadata": {}, "source": "### Joining Specific Columns"}, {"cell_type": "code", "execution_count": 56, "id": "c4cae2ed-3be1-4c56-89ff-39e5cb45bb2a", "metadata": {}, "outputs": [], "source": "full_order = full_order.join(tc.select('customer_id','customer_segment'),'customer_id','left')"}, {"cell_type": "code", "execution_count": 57, "id": "bfac32c9-f1a1-4334-8209-b61b35287ad1", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+----------------+\n|         customer_id|customer_segment|\n+--------------------+----------------+\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n|9ef432eb625129730...|       Low_Value|\n+--------------------+----------------+\nonly showing top 20 rows\n\n"}], "source": "full_order.select('customer_id','customer_segment').show()"}, {"cell_type": "markdown", "id": "fd2aec6c-d32b-49fc-ac98-b1831bdb0fca", "metadata": {}, "source": "## Date wise Orders Distribution"}, {"cell_type": "code", "execution_count": 61, "id": "4803eea2-9a62-43fa-9f73-b0525ff2b3f1", "metadata": {}, "outputs": [{"ename": "AttributeError", "evalue": "'NoneType' object has no attribute 'groupBy'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_3705/1578076337.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfull_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'order_purchase_timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'order_date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'order_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total_orders'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'order_date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfull_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'groupBy'"]}], "source": "from pyspark.sql.functions import *\n\nfull_order = full_order.groupBy(to_date('order_purchase_timestamp').alias('order_date')).agg(count('order_id').alias('total_orders')).orderBy('order_date')\nfull_order.show(3)"}, {"cell_type": "markdown", "id": "604a38b1-a5d8-48fb-80b9-c018d9af1884", "metadata": {}, "source": "## Hourly orders distribution"}, {"cell_type": "code", "execution_count": 28, "id": "96e409eb-81cd-4590-a60b-9552495a4d8e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 47:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+---------------+\n|hour|count(order_id)|\n+----+---------------+\n|   0|         423640|\n|   1|         209998|\n|   2|          90355|\n|   3|          47496|\n|   4|          40668|\n|   5|          28626|\n|   6|          84016|\n|   7|         213844|\n|   8|         528500|\n|   9|         873769|\n|  10|        1134498|\n|  11|        1202759|\n|  12|        1102917|\n|  13|        1152244|\n|  14|        1218318|\n|  15|        1159603|\n|  16|        1261538|\n|  17|        1152472|\n|  18|        1051446|\n|  19|        1109130|\n+----+---------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_order.groupBy(hour('order_purchase_timestamp').alias('hour')).agg(count('order_id')).orderBy('hour').show()"}, {"cell_type": "markdown", "id": "b75197de-0f70-4746-8891-88034d1f716d", "metadata": {}, "source": "## Weekdays Vs. Weekends"}, {"cell_type": "code", "execution_count": 79, "id": "ae7344b4-3064-4b3a-ba4f-2ca191c96209", "metadata": {}, "outputs": [], "source": "full_order = full_order.withColumn(\n    'day_type',\n    when(dayofweek('order_purchase_timestamp').isin(1, 7), lit('Weekend'))\n    .otherwise(lit('Weekday'))\n)"}, {"cell_type": "code", "execution_count": 34, "id": "5a1ebb7c-e044-4f79-9469-77c68288eecc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 51:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------+---------------+\n|day_type|count(order_id)|\n+--------+---------------+\n| Weekday|       13965495|\n| Weekend|        4098766|\n+--------+---------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_order.groupBy('day_type').agg(count('order_id')).show()"}, {"cell_type": "code", "execution_count": 80, "id": "3dd6d3e8-1338-40f5-b424-347262300bfc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------------+--------+--------------------+\n|order_purchase_timestamp|day_type|            order_id|\n+------------------------+--------+--------------------+\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n|     2017-10-02 10:56:33| Weekday|e481f51cbdc54678b...|\n+------------------------+--------+--------------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_order.select('order_purchase_timestamp','day_type','order_id').show()"}, {"cell_type": "markdown", "id": "67741885-f23d-4618-a3e6-90d9d8c40ce9", "metadata": {}, "source": "## Location wise Sales"}, {"cell_type": "code", "execution_count": null, "id": "3250bdee-f7da-46ef-a712-f51265c666d0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 55:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------------+---------------+\n|geolocation_state|count(order_id)|\n+-----------------+---------------+\n|               SC|         644944|\n|               RO|          24526|\n|               PI|          27693|\n|               AM|           6488|\n|               RR|           2411|\n|               GO|         162415|\n|             null|            317|\n|               TO|          22360|\n|               MT|         155225|\n|               SP|        6742207|\n|               ES|         367211|\n|               PB|          33379|\n|               RS|         971705|\n|               MS|          73679|\n|               AL|          37741|\n|               MG|        3433229|\n|               PA|          96276|\n|               BA|         443980|\n|               SE|          28145|\n|               PE|         132001|\n+-----------------+---------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_order.groupBy('geolocation_state').agg(count('order_id')).show()"}, {"cell_type": "markdown", "id": "d54bfc2e-afb5-4f50-b697-a74e9e2a8036", "metadata": {}, "source": "## Spark Configuration & Join Optimization Strategies"}, {"cell_type": "code", "execution_count": 37, "id": "6f245b55-fa1f-4a82-90cc-f633edcdc50d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/15 13:33:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://my-cluster-m.us-central1-a.c.keen-truth-461516-a0.internal:39921\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f9f7c64d900>"}, "execution_count": 37, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName('performance_optimization') \\\n    .config('spark.executor.memory', '6g') \\\n    .config('spark.executor.cores', '4') \\\n    .config('spark.executor.instances', '2') \\\n    .config('spark.driver.memory', '4g') \\\n    .config('spark.driver.maxResultSize', '2g') \\\n    .config('spark.sql.shuffle.partitions', '50') \\\n    .config('spark.default.parallelism', '50') \\\n    .config('spark.sql.adaptive.enabled', 'true') \\\n    .config('spark.sql.adaptive.coalescePartitions.enabled', 'true') \\\n    .config('spark.sql.autoBroadcastJoinThreshold', 20 * 1024 * 1024) \\      # Broadcast Size has been initialized as Required\n    .config('spark.sql.files.maxPartitionBytes', '64MB') \\\n    .config('spark.sql.files.openCostInBytes', '2MB') \\\n    .config('spark.memory.fraction', '0.8') \\\n    .config('spark.memory.storageFraction', '0.2') \\\n    .getOrCreate()\n\nspark"}, {"cell_type": "markdown", "id": "0299cd92-bedd-408f-b0d9-0c7525cd1ab8", "metadata": {}, "source": "## Broadcast : Good for Small Datasets"}, {"cell_type": "code", "execution_count": null, "id": "d9dd479c-23a4-486b-8d2d-e994e22733fd", "metadata": {}, "outputs": [], "source": "customer_broadcast = broadcast(customer_df)\noptimized_broadccast = full_order.join(customer_broadcast.'customer_id')"}, {"cell_type": "markdown", "id": "6e1c4666-28c1-42b5-8895-a6a1ecdd42bd", "metadata": {}, "source": "## Sort & Merge Join : Good for Large Datasets"}, {"cell_type": "code", "execution_count": null, "id": "65540b5a-a81d-410e-ae18-10e09fb32772", "metadata": {}, "outputs": [], "source": "sorted_customer = cutomer_df.sortWithinPartitions('customer_id')\nsorted_orders = full_order.sortWithinPartitions('customer_id')\n\nfull_merge = sorted_orders.join(sorted_customer, 'customer_id')"}, {"cell_type": "markdown", "id": "7054c373-324f-4c19-b8b5-055de78e8d84", "metadata": {}, "source": "## Bucket Join"}, {"cell_type": "code", "execution_count": null, "id": "26c126c9-6d53-41eb-baae-23e31af4bb27", "metadata": {}, "outputs": [], "source": "bucket_cust = customer_df.repartition(10,'cutomer_id')\nbucket_order = full_order.repartition(10,'customer_id')\n\nbucket_join = bucket_order.join(bucket_cust,'customer_id')"}, {"cell_type": "markdown", "id": "769bec65-094e-4cce-aa65-0b6048c9f6a7", "metadata": {}, "source": "## Skew Join Handling \n"}, {"cell_type": "code", "execution_count": null, "id": "bc8a5499-451e-427e-b5ae-ddfbcb4ee8e7", "metadata": {}, "outputs": [], "source": "skew_join = full_order.join(customer_df.hint('skew'),'customer_id')"}, {"cell_type": "markdown", "id": "a24228e0-a2a7-4b1a-84b6-123a69084b56", "metadata": {"tags": []}, "source": "# Saving Files"}, {"cell_type": "code", "execution_count": 47, "id": "5542d1c1-7a5d-4946-9a71-dabf495054a5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/15 15:15:14 WARN TaskSetManager: Lost task 1.0 in stage 60.0 (TID 72) (my-cluster-w-1.us-central1-a.c.keen-truth-461516-a0.internal executor 2): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:654)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:379)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: File does not exist: /data/olist/full/_temporary/0/_temporary/attempt_202506151512072482633446338519071_0060_m_000001_72/part-00001-729c84d4-90c8-4a2c-b80d-817507fa7e10-c000.snappy.parquet (inode 16560) Holder DFSClient_NONMAPREDUCE_-1044816377_36 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3103)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:610)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2977)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:912)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:595)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)\n\tat org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)\n\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/olist/full/_temporary/0/_temporary/attempt_202506151512072482633446338519071_0060_m_000001_72/part-00001-729c84d4-90c8-4a2c-b80d-817507fa7e10-c000.snappy.parquet (inode 16560) Holder DFSClient_NONMAPREDUCE_-1044816377_36 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3103)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:610)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2977)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:912)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:595)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1563)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1509)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1406)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)\n\tat com.sun.proxy.$Proxy34.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:531)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362)\n\tat com.sun.proxy.$Proxy35.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)\n\t... 3 more\n\n25/06/15 15:15:34 WARN TaskSetManager: Lost task 0.0 in stage 60.0 (TID 71) (my-cluster-w-0.us-central1-a.c.keen-truth-461516-a0.internal executor 1): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:654)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:379)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: File does not exist: /data/olist/full/_temporary/0/_temporary/attempt_202506151512072482633446338519071_0060_m_000000_71/part-00000-729c84d4-90c8-4a2c-b80d-817507fa7e10-c000.snappy.parquet (inode 16562) Holder DFSClient_NONMAPREDUCE_555057796_38 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3103)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:610)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2977)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:912)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:595)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)\n\tat org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)\n\tat org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)\n\tSuppressed: java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.error(ParquetFileWriter.java:217)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter$STATE.startBlock(ParquetFileWriter.java:209)\n\t\tat org.apache.parquet.hadoop.ParquetFileWriter.startBlock(ParquetFileWriter.java:407)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:184)\n\t\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:124)\n\t\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:164)\n\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:41)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseCurrentWriter(FileFormatDataWriter.scala:64)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:75)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.abort(FileFormatDataWriter.scala:117)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$2(FileFormatWriter.scala:366)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1550)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:369)\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$22(FileFormatWriter.scala:268)\n\t\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\t\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1505)\n\t\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/olist/full/_temporary/0/_temporary/attempt_202506151512072482633446338519071_0060_m_000000_71/part-00000-729c84d4-90c8-4a2c-b80d-817507fa7e10-c000.snappy.parquet (inode 16562) Holder DFSClient_NONMAPREDUCE_555057796_38 does not have any open files.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3103)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:610)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2977)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:912)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:595)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1563)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1509)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1406)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)\n\tat com.sun.proxy.$Proxy36.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:531)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362)\n\tat com.sun.proxy.$Proxy37.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)\n\t... 3 more\n\n                                                                                \r"}], "source": "# saving as parquet file in Data Proc\n\nfull_order.write.mode('overwrite').parquet('/data/olist/full')"}, {"cell_type": "code", "execution_count": 43, "id": "c5b26712-775b-431b-9e51-395cfd4b77c3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 3 items\n-rw-r--r--   2 root hadoop          0 2025-06-15 13:43 /data/olist/full/_SUCCESS\n-rw-r--r--   2 root hadoop  180868162 2025-06-15 13:43 /data/olist/full/part-00000-81316d82-c3c5-4db4-b96c-cb2b5e87ed2f-c000.snappy.parquet\n-rw-r--r--   2 root hadoop  109090406 2025-06-15 13:42 /data/olist/full/part-00001-81316d82-c3c5-4db4-b96c-cb2b5e87ed2f-c000.snappy.parquet\n"}], "source": "!hadoop fs -ls /data/olist/full/"}, {"cell_type": "markdown", "id": "a5cf0985-4fd4-4db3-be33-119d5620de4b", "metadata": {}, "source": "# Data Serving"}, {"cell_type": "code", "execution_count": 44, "id": "06fbbecc-7225-465b-bf59-42e4cdd936ae", "metadata": {}, "outputs": [], "source": "df = spark.read.parquet('/data/olist/full/')"}, {"cell_type": "code", "execution_count": 45, "id": "bd921299-4aa6-447c-a0e7-7e4953896158", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- order_status: string (nullable = true)\n |-- order_purchase_timestamp: timestamp (nullable = true)\n |-- order_approved_at: timestamp (nullable = true)\n |-- order_delivered_carrier_date: timestamp (nullable = true)\n |-- order_delivered_customer_date: timestamp (nullable = true)\n |-- order_estimated_delivery_date: timestamp (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n |-- product_category_name: string (nullable = true)\n |-- product_name_lenght: integer (nullable = true)\n |-- product_description_lenght: integer (nullable = true)\n |-- product_photos_qty: integer (nullable = true)\n |-- product_weight_g: integer (nullable = true)\n |-- product_length_cm: integer (nullable = true)\n |-- product_height_cm: integer (nullable = true)\n |-- product_width_cm: integer (nullable = true)\n |-- seller_zip_code_prefix: integer (nullable = true)\n |-- seller_city: string (nullable = true)\n |-- seller_state: string (nullable = true)\n |-- customer_unique_id: string (nullable = true)\n |-- customer_zip_code_prefix: integer (nullable = true)\n |-- customer_city: string (nullable = true)\n |-- customer_state: string (nullable = true)\n |-- geolocation_zip_code_prefix: integer (nullable = true)\n |-- geolocation_lat: double (nullable = true)\n |-- geolocation_lng: double (nullable = true)\n |-- geolocation_city: string (nullable = true)\n |-- geolocation_state: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- review_score: string (nullable = true)\n |-- review_comment_title: string (nullable = true)\n |-- review_comment_message: string (nullable = true)\n |-- review_creation_date: string (nullable = true)\n |-- review_answer_timestamp: string (nullable = true)\n |-- payment_sequential: integer (nullable = true)\n |-- payment_type: string (nullable = true)\n |-- payment_installments: integer (nullable = true)\n |-- payment_value: double (nullable = true)\n |-- day_type: string (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "markdown", "id": "c9c5662a-e9b1-4c19-a4ec-5c3d505b6c02", "metadata": {}, "source": "## Serving Data to Google Bucket"}, {"cell_type": "code", "execution_count": 130, "id": "eba61090-7590-4ad6-901a-1ddce5ebd775", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full_order.coalesce(1).write.mode('overwrite').parquet('gs://dataproc-staging-us-central1-912697423871-bztajijq/files/')"}, {"cell_type": "code", "execution_count": 109, "id": "3124f842-d5d9-4aed-a74f-08a5f4582b2a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: string (nullable = true)\n |-- order_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- order_status: string (nullable = true)\n |-- order_purchase_timestamp: timestamp (nullable = true)\n |-- order_delivered_customer_date: timestamp (nullable = true)\n |-- order_estimated_delivery_date: timestamp (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n |-- seller_zip_code_prefix: integer (nullable = true)\n |-- seller_city: string (nullable = true)\n |-- seller_state: string (nullable = true)\n |-- customer_unique_id: string (nullable = true)\n |-- customer_zip_code_prefix: integer (nullable = true)\n |-- customer_city: string (nullable = true)\n |-- customer_state: string (nullable = true)\n |-- geolocation_zip_code_prefix: integer (nullable = true)\n |-- geolocation_city: string (nullable = true)\n |-- geolocation_state: string (nullable = true)\n |-- review_id: string (nullable = true)\n |-- review_score: string (nullable = true)\n |-- payment_type: string (nullable = true)\n |-- payment_value: double (nullable = true)\n |-- total_spent: double (nullable = true)\n |-- total_orders: long (nullable = true)\n |-- AOV: double (nullable = true)\n |-- day_type: string (nullable = false)\n |-- customer_segment: string (nullable = true)\n\n"}], "source": "full_order.printSchema()"}, {"cell_type": "code", "execution_count": null, "id": "355a0ed5-a095-47e4-b675-405de6d0a431", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}